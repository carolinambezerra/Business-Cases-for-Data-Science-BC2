{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7ece3bc",
   "metadata": {},
   "source": [
    "# **Business Cases Course**\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Group Project 2022/2023**\n",
    "- **Academic Year: 2020-2023 | 2nd Semester**\n",
    "- **Professor: Nuno AntÃ³nio**\n",
    "\n",
    "<br>\n",
    "\n",
    "- **\"Case 2: Market Basket Analysis\"**\n",
    "- **Asian Food Restaurant from Company C**\n",
    "- **This notebook uses the *Case2_AsianRestaurant_Cyprus_2018.txt* dataset**\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "> **Group composed by:**<p>\n",
    "> Ana Carolina Ottavi, nÂº 20220541<p>\n",
    "> Carolina Bezerra, nÂº 20220392 <p>\n",
    "> Duarte GirÃ£o, nÂº 20220670<p>\n",
    "> JoÃ£o PÃ³lvora, nÂº 20221037<p>\n",
    "> Luca Loureiro, nÂº 20221750<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5650f72",
   "metadata": {},
   "source": [
    "## ðŸ“– Introduction\n",
    "    \n",
    "Within the scope of __Business Cases for Data Science__, it was proposed a project, where the groups' ability to deliver a market basket analysis in accordance with the different features included in a dataset containing 84.109 records would be tested. However, the company C that ownes this restaurant realized that they are not having enough profits and therefore, wishes to better understand their customers and its preferences.\n",
    "    \n",
    "Therefore, the business goal determined for this project was to develop a market basket analysis. C expects to gain insights in terms of creation of menus, introduction of new products, understand substitute products, recommending/promoting cross-selling, customer segmentation and other possible results depending on the findings. \n",
    "\n",
    "C has several questions that it hopes to get answers to:\n",
    "- Are there any differences between dine-inn customers and delivery customers?\n",
    "- Is the product offering adequate (e.g., do customers make strange combination of products) ?\n",
    "- Are there any patterns in consumption that may indicate tendencies?\n",
    "\n",
    "<br>\n",
    "\n",
    "## ðŸ“–Dataset description\n",
    "\n",
    "This notebook uses the Case2_AsianRestaurant_Cyprus_2018.txt. The Dataset is related with all sales transactions of a restaurant located in Nicosia, the capital of Cyprus.<br> This asian food restaurant is inserted in a company C, with restaurants spread all over the this country. <br> This restaurant is struggling to maintain its profit margin and continuous growth due to increasing competition and customersâ€™ changes in habits. To try to revert this process, C wants to take advantage of its sales data to understand customersâ€™ patterns of consumption and preferences.\n",
    "\n",
    "The dataset is composed of the following columns:\n",
    "- **DocNumber**: number of the document. The document number repeats in as many rows as the rows in the original document (invoice)\n",
    "- **ProductDesignation**: product designation\n",
    "- **ProductFamily**: name of the family of the product. A product can only be member of one only family\n",
    "- **Qty**: quantity\n",
    "- **TotalAmount**: sale price of the total quantity\n",
    "- **InvoiceDateHour**: date and hour when the document was issued\n",
    "- **EmployeeID**: ID of the employee who issued the document\n",
    "- **IsDelivery**: indication if sale was a delivery or a dine-inn (1:\n",
    "delivery, 0: dine-inn)\n",
    "- **Pax**: number of persons at the table\n",
    "- **CustomerID**: ID of the customer (if a customer record was\n",
    "assigned to the sale)\n",
    "- **CustomerCity**: city of the customer (usually only employed in\n",
    "delivery)\n",
    "- **CustomerSince**: date of creation of the customer\n",
    "\n",
    "Note: Each row in the dataset represents a document line (invoice line)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f8d3c7",
   "metadata": {},
   "source": [
    "# **1. Initial Setup**\n",
    "\n",
    "- 1.1) Importing needed libraries\n",
    "- 1.2) Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcc2e45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install in case its necessary\n",
    "    # pip install mlxtend\n",
    "    # pip install networkx\n",
    "    # pip install WorldCloud\n",
    "    # pip install geopy\n",
    "    # pip install folium\n",
    "    \n",
    "# Imports\n",
    "import category_encoders as ce\n",
    "import datetime as dt\n",
    "import folium\n",
    "import joypy\n",
    "import itertools\n",
    "import math\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import re\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import warnings\n",
    "import ydata_profiling\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# From mlxtend\n",
    "# Associated Rules related imports\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "# From other libraries\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import parser\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from geopy import exc\n",
    "from itertools import cycle,islice, product\n",
    "from math import ceil\n",
    "from matplotlib import ticker\n",
    "from plotly.subplots import make_subplots\n",
    "from os.path import join\n",
    "from ydata_profiling import ProfileReport\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Setting Visual Theme\n",
    "sns.set_theme(style = 'white', palette = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e834294",
   "metadata": {},
   "source": [
    "# **2. Data Understanding**\n",
    "\n",
    "- **2.1) Collect Initial Data: Reading the original file**\n",
    "- 2.1.1) Reading the original file\n",
    "- 2.1.2) Making a safety copy of the dataset\n",
    "- **2.3) Describe Data: General description of data**\n",
    "- **2.4) Explore Data**\n",
    "- 2.4.1) Counting the number of rows and columns \n",
    "- 2.4.2) Checking the top and bottom from the dataset\n",
    "- 2.4.3) Checking Data types\n",
    "- 2.4.4) Describe Data: General description of data after changing dtypes\n",
    "- 2.4.5) Checking the number of unique values per feature\n",
    "- **2.6) Verify Data Quality**\n",
    "- 2.6.1) Checking Duplicated Values\n",
    "- 2.6.2) Checking Missing Values\n",
    "- 2.6.3) Checking CustomerSince and InvoiceDateHour\n",
    "- 2.6.4) Checking DocNumber\n",
    "- 2.6.5) Checking ProductDesignation and ProductFamily\n",
    "- 2.6.6) Checking Qty and Total Amount\n",
    "- 2.6.7) Checking EmployeeID\n",
    "- 2.6.8) Checking Pax and CustomerID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07a4dad",
   "metadata": {},
   "source": [
    "- **2.1) Collect Initial Data: Reading the original file**\n",
    "- 2.1.1) Reading the original file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc2d155e",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\Users\\\\cmade\\\\OneDrive\\\\Ambiente de Trabalho\\\\BC2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mdtypes = {'DocNumber':'category',\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m          'ProductDesignation':'category', \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m          'CustomerSince':'category'}\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03mdf = pd.DataFrame(pd.read_csv(\"Case2_AsianRestaurant_Cyprus_2018.txt\",sep=\";\", dtype=dtypes))'''\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Reading the dataframe.\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mcmade\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mOneDrive\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mAmbiente de Trabalho\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mBC2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     18\u001b[0m df\u001b[38;5;241m.\u001b[39minfo()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:\\\\Users\\\\cmade\\\\OneDrive\\\\Ambiente de Trabalho\\\\BC2'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "dtypes = {'DocNumber':'category',\n",
    "          'ProductDesignation':'category', \n",
    "          'ProductFamily':'category',\n",
    "          'Qty':'category', \n",
    "          'TotalAmount': 'category',\n",
    "          'InvoiceDateHour':'category',\n",
    "          'EmployeeID':'category',\n",
    "          'IsDelivery':'category',\n",
    "          'Pax':'category',\n",
    "          'CustomerID':'category',\n",
    "          'CustomerCity':'category',\n",
    "          'CustomerSince':'category'}\n",
    "df = pd.DataFrame(pd.read_csv(\"Case2_AsianRestaurant_Cyprus_2018.txt\",sep=\";\", dtype=dtypes))'''\n",
    "\n",
    "# Reading the dataframe.\n",
    "df = pd.DataFrame(pd.read_csv(\"C:\\\\Users\\\\cmade\\\\OneDrive\\\\Ambiente de Trabalho\\\\BC2\",sep=\";\"))\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c71837",
   "metadata": {},
   "source": [
    "- **2.1) Collect Initial Data: Reading the original file**\n",
    "- 2.1.2) Making a safety copy of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24c5c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a first security copy of our original dataset.\n",
    "\n",
    "df_original = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7419e73a",
   "metadata": {},
   "source": [
    "- **2.3) Describe Data: General description of data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c6d306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check descriptive statistics before replacing missing values.\n",
    "# Here, we can have a good sensibility on data.\n",
    "\n",
    "df.describe(include=\"all\").T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31be8fd7",
   "metadata": {},
   "source": [
    "**Key Takeawyays**\n",
    "- There are repeated DocNumbers, which is possible since each DocNumber is repeated for how many products there are in the invoice. The restaurant has 11.147 invoices for the year of 2018.\n",
    "- There are 255 different product designations and 27 different product families. Mineral water was the most requested product (7061 times). Starters is the most requested family of products (14.148 times).\n",
    "- Most people order 1 unit of each product.\n",
    "- The most ordered products have a cost of 3 to the customer over the total quantity. However there's a wrong ponctuation for this column - there shouldnt be a comma, instead, it should be a dot.\n",
    "- The most busy day in terms of hours is the 24th of December of 2018, which is the Christmas Eve. Makes sense that more people come in the holidays.\n",
    "- There isn't an employee 1 in the restaurant and theres an employee 27. Employee 2 takes the most orders\n",
    "- Most orders are for dine-in\n",
    "- The majority of tables answered have only 1 person and the maximum value for pax is 200\n",
    "- Most customers don't have an ID\n",
    "- Customers registered are from 17 different cities and most cusotmers are from Egkomi, which is a suburb and municipality of the Cypriot capital Nicosia\n",
    "- The customers of this restaurant have been customers of it for a long while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852e93c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualizing the profilling of our dataset to have a general sensibility on data.\n",
    "# df_profilling =ydata_profiling.ProfileReport(df)\n",
    "# partial_df_profilling =ydata_profiling.ProfileReport(partial_df)\n",
    "# df_profilling. to_file(\"df_report.html\")\n",
    "# partial_df_profilling. to_file(\"partial_df_report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb01266c",
   "metadata": {},
   "source": [
    "- **2.4) Explore Data**\n",
    "- 2.4.1) Counting the number of rows and columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c5e882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows and columns.\n",
    "# Get more sensibility on data.\n",
    "    # 84109 rows (records)\n",
    "    # 12 columns (variables)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b457ac3",
   "metadata": {},
   "source": [
    "- **2.4) Explore Data**\n",
    "- 2.4.2) Checking the top and bottom from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fdfc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the first two on top and on the bottom of the dataset df\n",
    "# Is there a difference between TK and TKD?\n",
    "\n",
    "df_sliced = df.head(2)\n",
    "df_sliced = df_sliced.append(df.tail(2))\n",
    "df_sliced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb95ff8",
   "metadata": {},
   "source": [
    "- **2.4) Explore Data**\n",
    "- 2.4.3) Data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3799139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset data types.\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9984a068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we are changing the three datatypes.\n",
    "# There was an error converting dtype of 'TotalAmount' from object to float because there was a comma (,)\n",
    "# instead of (.) for decimals units.\n",
    "\n",
    "df.TotalAmount = df.TotalAmount.apply(lambda x:float(str(x.replace(',','.'))))\n",
    "df['InvoiceDateHour'] = pd.to_datetime(df['InvoiceDateHour'])\n",
    "df['CustomerSince'] = pd.to_datetime(df['CustomerSince'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8499ff4b",
   "metadata": {},
   "source": [
    "- **2.4) Explore Data**\n",
    "- 2.4.4) Describe Data: General description of data after changing dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4d6ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General sensibility on data.\n",
    "\n",
    "df.describe(include=\"all\").T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4232e02c",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "- The mean of total amount of value paid for a specific quantity of a product is around 9.83 and the max is 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5b0121",
   "metadata": {},
   "source": [
    "- **2.4) Explore Data**\n",
    "- 2.4.5) Checking the number of unique values per feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd32ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of unique values per feature.\n",
    "# Check each feature cardinality.\n",
    "\n",
    "df.nunique().to_frame(name = 'Number of unique values per feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1247592",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get unique values for all the features\n",
    "\n",
    "columns = df.columns.to_list()\n",
    "\n",
    "for column in columns:\n",
    "    #print(\"\\n\")\n",
    "    print(\"Column Name:\", column)\n",
    "    print(df[column].unique())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68c5d9f",
   "metadata": {},
   "source": [
    "- **2.6) Verify Data Quality**\n",
    "- 2.6.1) Checking Duplicated Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010d1062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the duplicated values on our dataset.\n",
    "# There 3923 duplicated records.\n",
    "\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51e7219",
   "metadata": {},
   "source": [
    "- **2.6) Verify Data Quality**\n",
    "- 2.6.2) Checking Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2542aab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represent a missing values in pandas with a NaN value.\n",
    "\n",
    "df.replace(\"\", np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652f23dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the missing values on our dataset.\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7540154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the number of null values for the group of deliveries and for the total. \n",
    "\n",
    "print(df.loc[df[\"IsDelivery\"]==0, ].isnull().sum()) # Representing Dine-Inns.\n",
    "print(df.loc[df[\"IsDelivery\"]>0, ].isnull().sum()) # Representing Deliveries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f72310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a visual look at the missing values per variable.\n",
    "# We can easily check that the variables 'CustomerCity' and 'CustomerSince' have a significant amount of missing values.\n",
    "\n",
    "msno.bar(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d78814",
   "metadata": {},
   "source": [
    "- **2.6) Verify Data Quality**\n",
    "- 2.6.3) Checking CustomerSince and InvoiceDateHour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88b86a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# InvoiceDateHour\n",
    "# Checking the unique values of this variable.\n",
    "\n",
    "df['InvoiceDateHour'].sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd95446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustomerSince\n",
    "# Checking the unique values of this variable.\n",
    "# This plataform exists since 2005, which is the date of the oldest client.\n",
    "\n",
    "df['CustomerSince'].sort_values().unique(), \n",
    "print('The total amount of impossible values is:', df[df['CustomerSince']>'2018-12-31 23:59:59'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16dac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under our understanding, it is impossible that CustomerSince is higher than InvoiceDateHour:\n",
    "    # InvoiceDateHour: date and hour when the document was issued.\n",
    "    # CustomerSince: date the customer was created.\n",
    "    \n",
    "df[df['InvoiceDateHour'] < df['CustomerSince']].shape[0]\n",
    "print('The total amount of impossible values is:', df[df['InvoiceDateHour'] < df['CustomerSince']].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8460fe",
   "metadata": {},
   "source": [
    "- **2.6) Verify Data Quality**\n",
    "- 2.6.4) Checking DocNumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da129a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DocNumber\n",
    "# Checking the main values amount of this variables.\n",
    "# Some invoices include orders with a lot of different products\n",
    "\n",
    "df['DocNumber'].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befe223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's WITH Product Family?\n",
    "# Getting our understanding of ProductFamily per DocNumber.\n",
    "\n",
    "pd.crosstab(df['DocNumber'], df['ProductFamily'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30af8328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why are there products registered that has a TotalAmount=0 or TotalAmount equal to a few cents?\n",
    "# Getting our understanding of TotalAmount per DocNumber.\n",
    "\n",
    "pd.crosstab(df['DocNumber'], df['TotalAmount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c6a5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we know employee that number of employees equal to ONE doesn't exist.\n",
    "# The employees working in the restaurant at 2018 are designated with the nrs. 2, 4, 5, 6, 7, 23 and 27.\n",
    "# Getting our understanding of EmployeeID per DocNumber.\n",
    "\n",
    "pd.crosstab(df['DocNumber'], df['EmployeeID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0029149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fact that TK DocNumber have numbers when IsDelivery=0, \n",
    "# makes us believe that TK is code used for dine-ins and TKD is code used for deliveries.\n",
    "# Getting our understanding of IsDelivery per DocNumber.\n",
    "\n",
    "pd.crosstab(df['DocNumber'], df['IsDelivery'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005c651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting our understanding of the relationship between 'IsDelivery' and 'CustomerID'.\n",
    "# We can check that all the times that the 'CustomerID' has no value, means that it is a 'DineIn' and not a delivery.\n",
    "\n",
    "df['CustomerID'] = df['CustomerID'].astype('str')\n",
    "df_deliveries = pd.crosstab(index=np.where(df['CustomerID']>'0','1','0'), columns=df['IsDelivery'])\n",
    "df_deliveries.index=['Invoice with no customer ID','Invoice with customer ID']\n",
    "df_deliveries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35acff8d",
   "metadata": {},
   "source": [
    "- **2.6) Verify Data Quality**\n",
    "- 2.6.5) Checking ProductDesignation and ProductFamily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca3c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a close look at the relationship between ProductDesignation, ProductFamily and TotalAmount.\n",
    "# Now, we can see that extras total amount is equal to 0, as well as holds equals to no meat. But also other products, \n",
    "# which dont make that much sense.\n",
    "# We also can tell that delivery has a charge, which counts as extras\n",
    "\n",
    "#df.groupby('TotalAmount')['ProductDesignation', 'ProductFamily'].sum()\n",
    "df.groupby(['ProductDesignation', 'ProductFamily'])['TotalAmount'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4264646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a close look at the relationship between ProductFamily and TotalAmount.\n",
    "# Now, we can see that extras total amount is equal to 0, but also other products, which dont make that much sense\n",
    "\n",
    "categorical_table = pd.crosstab(index=df['ProductFamily'], columns=df['TotalAmount'])\n",
    "categorical_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425ae04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'With' and 'Holds' PorductFamily have no price\n",
    "\n",
    "concat_df = pd.concat([df.groupby([\"ProductFamily\"])[\"Qty\"].sum(), \n",
    "                       df.groupby([\"ProductFamily\"])[\"TotalAmount\"].sum()], axis=1)\n",
    "display(concat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d36332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lowercase column using str.lower().\n",
    "df['ProductFamily'] = df['ProductFamily'].str.lower()\n",
    "\n",
    "# Capitalize first letter.\n",
    "df['ProductFamily'] = df['ProductFamily'].str.capitalize()\n",
    "\n",
    "# Setting visual theme\n",
    "sns.set_theme(style = 'white', palette = None)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.countplot(\n",
    "                data = df,\n",
    "                y = df['ProductFamily'],\n",
    "                color='lightsteelblue',\n",
    "                edgecolor='lightsteelblue',\n",
    "                order=df.ProductFamily.value_counts().iloc[:10].index\n",
    "             )\n",
    "# Layout\n",
    "plt.suptitle('ProductFamily Frequency', fontsize = 15, fontweight = 'bold')\n",
    "plt.legend(title='', loc='lower right', prop={'size': 10})\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c416b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lowercase column using str.lower()\n",
    "df['ProductDesignation'] = df['ProductDesignation'].str.lower()\n",
    "\n",
    "# Capitalize first letter\n",
    "df['ProductDesignation'] = df['ProductDesignation'].str.capitalize()\n",
    "\n",
    "# Getting sensibility on all 'ProductDesignation'.\n",
    "Products = list(df.ProductDesignation.unique())\n",
    "display(Products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84831abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting visual theme\n",
    "sns.set_theme(style = 'white', palette = None)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.countplot(\n",
    "                data = df,\n",
    "                y = df['ProductDesignation'],\n",
    "                color='lightsteelblue',\n",
    "                edgecolor='lightsteelblue',\n",
    "                order=df.ProductDesignation.value_counts().iloc[:10].index\n",
    "             )\n",
    "# Layout\n",
    "plt.suptitle('ProductDesignation', fontsize = 15, fontweight = 'bold')\n",
    "plt.legend(title='', loc='lower right', prop={'size': 10})\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa89657c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualization of the 20 main products ordered by costumers.\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "wordcloud = WordCloud(background_color = 'white', \n",
    "                      width = 1400,  \n",
    "                      height = 1000, \n",
    "                      max_words = 20).generate(str(df['ProductDesignation']))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.title('Most Popular Items bought first by the Customers:',\n",
    "          fontsize = 20, \n",
    "          fontweight = 'bold', \n",
    "          x= 0.37, \n",
    "          y=1.08)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1973e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under all 'ProductDesignation', understanding what are those that have a 'NO' on the name.\n",
    "no_list = []\n",
    "for no_product in Products:\n",
    "    if ' no ' in no_product:\n",
    "        product_NO = no_product.strip()\n",
    "        no_list.append(product_NO)\n",
    "display(no_list)\n",
    "\n",
    "\n",
    "\n",
    "# Under all 'ProductDesignation', understanding what are those that have an 'EXTRA' on the name.\n",
    "extra_list = []\n",
    "for extra_product in Products:\n",
    "    if ' extra ' in extra_product:\n",
    "        product_EXTRA = extra_product.strip()\n",
    "        extra_list.append(product_EXTRA)\n",
    "display(extra_list)\n",
    "\n",
    "\n",
    "\n",
    "# Under all 'ProductDesignation', understanding what are those that have an 'WITH' on the name.\n",
    "with_list = []\n",
    "for with_product in Products:\n",
    "    if ' with ' in with_product:\n",
    "        product_WITH = with_product.strip()\n",
    "        with_list.append(product_WITH)\n",
    "display(with_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed900aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This graph below is very good to obtain sensibility throughout the existing families.\n",
    "    # It tell us the main preferences of our customers and we can use this information in the end for our market analysis.\n",
    "# We can see below that in deliveries, there is a tendency for clients to personalize there requests with additional\n",
    "# products.\n",
    "\n",
    "# ProductFamily Consumption - Dinne-Inns \n",
    "df_dinne_inns = df[df.IsDelivery == 0][[\"ProductFamily\",\"Qty\"]].groupby([\"ProductFamily\"]).sum().sort_values(by = \"Qty\", ascending = False).reset_index() \n",
    "\n",
    "# ProductFamily Consumption - deliveries (excluding delivery charges)\n",
    "df_delivery = df[df.ProductDesignation != \"DELIVERY CHARGE\"][df.IsDelivery == 1][[\"ProductFamily\",\"Qty\"]].groupby([\"ProductFamily\"]).sum().sort_values(by = \"Qty\", ascending = False).reset_index() \n",
    "df_merged = df_dinne_inns.merge(df_delivery, left_on= \"ProductFamily\", right_on=\"ProductFamily\" ) \n",
    " \n",
    "# Plot    \n",
    "fig = go.Figure(data=[ \n",
    "    go.Bar(marker=dict(color='lightsteelblue'),name='Dinne-Inns', x=df_merged['ProductFamily'], y=df_merged['Qty_x']), \n",
    "    go.Bar(marker=dict(color='seagreen'),name='Deliveries', x=df_merged['ProductFamily'], y=df_merged['Qty_y']) \n",
    "]) \n",
    "\n",
    "# Change the bar mode \n",
    "fig.update_layout(barmode=\"group\",\n",
    "                  height=450, \n",
    "                  width=950, \n",
    "                  title_text=\"Bestselling products group by families (In Absolute Values)\", \n",
    "                  xaxis_title=\"Product Family\",\n",
    "                  yaxis_title=\"Quantity (Qty)\",\n",
    "                  font=dict(size=16, color='black'),\n",
    "                  plot_bgcolor=\"white\")\n",
    "fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d844042e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main conclusions from the graphs below:\n",
    "    # Spring Roll is the most requested product either for Dinne-Inns as for deliveries. \n",
    "\n",
    "df_dinne_inns = df[df.IsDelivery == 0][[\"ProductDesignation\",\"Qty\"]].groupby([\"ProductDesignation\"]).sum().sort_values(by = \"Qty\", ascending = False).reset_index() \n",
    "df_delivery = df[df.IsDelivery == 1][[\"ProductDesignation\",\"Qty\"]].groupby([\"ProductDesignation\"]).sum().sort_values(by = \"Qty\", ascending = False).reset_index() \n",
    "\n",
    "\n",
    "# Plot\n",
    "fig = make_subplots(1,2, subplot_titles=[\"Dinne-Inns\", \"Delivery\"])\n",
    "fig.add_trace(go.Bar(marker=dict(color='lightsteelblue'),\n",
    "                     name='Dinne-Inns', \n",
    "                     x=df_dinne_inns['ProductDesignation'][0:20], \n",
    "                     y=df_dinne_inns[\"Qty\"][0:20]),1,1)\n",
    "\n",
    "# We are always excluding delivery charge products\n",
    "fig.add_trace(go.Bar(marker=dict(color='seagreen'),\n",
    "                     name='Deliveries', \n",
    "                     x=df_delivery[df_delivery['ProductDesignation']!= \"DELIVERY CHARGE\"]['ProductDesignation'][0:20], \n",
    "                     y=df_delivery[df_delivery['ProductDesignation']!= \"DELIVERY CHARGE\"][\"Qty\"][0:20]),1,2)\n",
    "fig.update_layout(height=450, \n",
    "                  width=950, \n",
    "                  title_text=\"Bestselling products\", \n",
    "                  xaxis_title=\"Product Designation\",\n",
    "                  yaxis_title=\"Quantity (Qty)\",\n",
    "                  plot_bgcolor=\"white\")\n",
    "fig.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b5f3a3",
   "metadata": {},
   "source": [
    "- **2.6) Verify Data Quality**\n",
    "- 2.6.6) Checking Qty and Total Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce669981",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Understanding the most required quantities.\n",
    "\n",
    "df['Qty'].value_counts()/ len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bda81de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'IsDelivery' distributed per 'Qty' and 'TotalAmount'.\n",
    "# We can say that the restaurant gains less money with deliveries than with dine-ins.\n",
    "\n",
    "concat_df_2 = pd.concat([df.groupby([\"IsDelivery\"])[\"Qty\"].sum(), \n",
    "                       df.groupby([\"IsDelivery\"])[\"TotalAmount\"].sum()], axis=1)\n",
    "display(concat_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f68c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing the variable 'IsDelivery' graphically.\n",
    "\n",
    "# Draw\n",
    "fig, ax = plt.subplots(figsize=(7,6))\n",
    "g = sns.countplot(data=df, \n",
    "                  x=df['IsDelivery'], \n",
    "                  color = 'lightsteelblue')\n",
    "# Decoration\n",
    "fmt = \"{x:,.0f}\"\n",
    "tick = ticker.StrMethodFormatter(fmt)\n",
    "ax.yaxis.set_major_formatter(tick)\n",
    "sns.despine()\n",
    "\n",
    "# Plot\n",
    "plt.suptitle('Frequency of IsDelivery', fontsize = 15, fontweight = 'bold')\n",
    "plt.xlabel(\"IsDelivery value\")\n",
    "plt.ylabel(\"Quantity\")\n",
    "plt.rc('axes', labelsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d2ea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'CustomerCity' distributed per 'Qty' and 'TotalAmount'.\n",
    "\n",
    "concat_df_3 = pd.concat([df.groupby([\"CustomerCity\"])[\"Qty\"].sum(), \n",
    "                       df.groupby([\"CustomerCity\"])[\"TotalAmount\"].sum()], axis=1)\n",
    "display(concat_df_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f78e97",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Convert lowercase column using str.lower()\n",
    "df['CustomerCity'] = df['CustomerCity'].str.lower()\n",
    "\n",
    "# Capitalize first letter\n",
    "df['CustomerCity'] = df['CustomerCity'].str.capitalize()\n",
    "\n",
    "# Setting visual theme\n",
    "sns.set_theme(style = 'white', \n",
    "              palette = None)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(11,8))\n",
    "sns.countplot(data = df,\n",
    "              y = df['CustomerCity'],\n",
    "              color='lightsteelblue',\n",
    "              edgecolor='lightsteelblue',\n",
    "              order = df['CustomerCity'].value_counts().index\n",
    "              )\n",
    "# Layout\n",
    "plt.suptitle('CustomerCity', \n",
    "             fontsize = 20, \n",
    "             fontweight = 'bold')\n",
    "plt.legend(title='Absolute Values', \n",
    "           loc='lower right', \n",
    "           prop={'size': 14})\n",
    "plt.xlabel('Number of Customers', fontsize=14)\n",
    "plt.ylabel('City/Place/District', fontsize=14)\n",
    "plt.grid(axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee83f51",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Below, we can have a deeper understanding to answer the following question:\n",
    "    # \"Do more orders correspond to more revenue?\"\n",
    "        # And we can see below, costumer spend, on average, more than the double on restaurant than by deliveries.\n",
    "\n",
    "# Defining variables.\n",
    "revenue_dinne_inn = df[df.IsDelivery == 0].TotalAmount.sum()\n",
    "revenue_delivery = df[df.IsDelivery == 1].TotalAmount.sum()\n",
    "\n",
    "# Counting.\n",
    "ordered_dinne_inn = len(df[df.IsDelivery == 0].DocNumber.unique()) # Counting the number of records in dinne-inn.\n",
    "ordered_delivery = len(df[df.IsDelivery == 1].DocNumber.unique())  # Counting the number of records by delivery.\n",
    "\n",
    "# Plot.\n",
    "fig = make_subplots(1,2, \n",
    "                    specs=[[{\"type\":\"domain\"},\n",
    "                            {\"type\":\"domain\"}]], \n",
    "                    subplot_titles=[\"Revenue\", \"Number of orders\"])\n",
    "colors = ['lightsteelblue', 'seagreen']\n",
    "fig.add_trace(go.Pie(labels=[\"Dinne-Inn\", \"Deliveries\"], \n",
    "                     values=[revenue_dinne_inn, revenue_delivery],\n",
    "                     #textinfo= 'value+percent',\n",
    "                     marker=dict(colors=colors)),1,1)\n",
    "fig.add_trace(go.Pie(values=[ordered_dinne_inn, ordered_delivery],\n",
    "                     marker=dict(colors=colors),\n",
    "                     showlegend=False),1,2)\n",
    "fig.update_layout(height=500, \n",
    "                  width=650, \n",
    "                  title_text=\"Dinne-Inn vs Deliveries\",\n",
    "                  font=dict(size=12))\n",
    "\n",
    "# Average Spending per request. \n",
    "average_spending_dinneinns = np.mean(df[df.IsDelivery == 0][[\"DocNumber\",\"TotalAmount\"]].groupby(\"DocNumber\").sum()) \n",
    "average_spending_deliveries = np.mean(df[df.IsDelivery == 1][[\"DocNumber\",\"TotalAmount\"]].groupby(\"DocNumber\").sum()) \n",
    "print(\"Average Dinne-Inns spending: \" + str(round(average_spending_dinneinns[0],2)) + \" Euros\" \"\\n\" + \n",
    "      \"Average delivery spending: \" + str(round(average_spending_deliveries[0],2))+ \" Euros\") \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fa8c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we aim to understand how many times normally customers use to visit this resturants chain.\n",
    "# Most preferences:\n",
    "    # 5-10 visits: 41%\n",
    "    # 10+ visits: ~36%\n",
    "    # 3-5 visits: 22%\n",
    "\n",
    "on_cust = df[df.IsDelivery == 1]\n",
    "colors = [\"lightsteelblue\",\"seagreen\",\"darkgreen\",\"red\",\"yellow\"]\n",
    "freq = on_cust.CustomerID.value_counts()\n",
    "freq_bins = pd.Series([\"1\" if i == 1 else \n",
    "                       \"2\" if i == 2 else \n",
    "                       \"3-5\" if i <= 5 else \n",
    "                       \"5-10\" if i <= 10 else \n",
    "                       \"10+\" for i in freq.values])\n",
    "\n",
    "# Plot\n",
    "fig = go.Figure(go.Pie(marker=dict(colors=colors),\n",
    "                       labels=freq_bins.value_counts().index, \n",
    "                       textinfo='percent',\n",
    "                       values=freq_bins.value_counts().values))\n",
    "fig.update_layout(height=500, \n",
    "                  width=700, \n",
    "                  legend_title = \"Different number of visits gaps\",\n",
    "                  title_text = \"Distribution of the number of visits per client\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1c7a80",
   "metadata": {},
   "source": [
    "- **2.6) Verify Data Quality**\n",
    "- 2.6.7) Checking EmployeeID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8243c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['EmployeeID'].value_counts()/ len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8679e0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['IsDelivery'], df['EmployeeID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01767497",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df_3 = pd.concat([df.groupby([\"EmployeeID\"])[\"Qty\"].sum(), \n",
    "                       df.groupby([\"EmployeeID\"])[\"TotalAmount\"].sum()], axis=1)\n",
    "display(concat_df_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ae4b5e",
   "metadata": {},
   "source": [
    "- **2.6) Verify Data Quality**\n",
    "- 2.6.8) Checking Pax and CustomerID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2e05e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer equal to 0 may be what is inserted when the client doesn't want to create a client account in the restaurant.\n",
    "# Human error, so we will keep it for now.\n",
    "\n",
    "df['CustomerID'].value_counts()/ len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b17053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some records with pax=0. \n",
    "# Since it has a very low representation in the dataset, we consider it a human error. \n",
    "# It can perfectly happen that an employee did not take note of the number of costumers of the respective order.\n",
    "\n",
    "df['Pax'].value_counts()/ len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c1445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we aim to obtain more sensibility on how many people usually come to the restaurant.\n",
    "# As we can see, the customers tend to organize themselves to come either in groups of 2 (pairs) \n",
    "# or in groups of 4 (prossibility a family).\n",
    "\n",
    "x = df[(df.IsDelivery == 0) & (df.Pax < 25)].Pax.values\n",
    "\n",
    "# Plot\n",
    "fig = px.histogram(x=x)\n",
    "fig.update_traces(marker_color='lightsteelblue', \n",
    "                  opacity=0.7)\n",
    "fig.update_layout(height=450, \n",
    "                  width=950,\n",
    "                  yaxis_title=\"Frequency (Absolute Values)\", \n",
    "                  xaxis_title=\"Number of persons\", \n",
    "                  title_text=\"Number of customers per meal\", \n",
    "                  showlegend=False, \n",
    "                  plot_bgcolor=\"white\")\n",
    "mean_value = df['Pax'].mean()\n",
    "fig.add_vline(x=mean_value, \n",
    "              line_dash=\"dash\", \n",
    "              line_color=\"seagreen\", \n",
    "              annotation_text=f\"Mean: {mean_value:.2f}\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79df9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding ou 'TotalAmount' varies with 'Pax' and 'CustomerID'.\n",
    "\n",
    "df.groupby('TotalAmount')['Pax', 'CustomerID'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72f81c2",
   "metadata": {},
   "source": [
    "# **3. Data Preparation**\n",
    "\n",
    "- **3.1) Selecting data**\n",
    "- 3.1.1) Removing unnecessary rows from Data Exploration\n",
    "- 3.1.2) Removing unnecessary features from Data Exploration\n",
    "- 3.1.3) Defining metric and categorical features\n",
    "- **3.2) Cleaning data**\n",
    "- 3.2.1) Dealing with missing values and strange values\n",
    "- 3.2.2) Histograms Analysis\n",
    "- 3.2.3) Boxplots Analysis\n",
    "- 3.2.3.1) Manual Removal\n",
    "- 3.2.3.2) IQR Removal\n",
    "- 3.2.4) Correlations Analysis\n",
    "- **3.3) Construct data**\n",
    "- 3.3.1) Creating new variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef314cfd",
   "metadata": {},
   "source": [
    "- **3.1) Selecting data**\n",
    "- 3.1.1) Removing unnecessary rows from Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935f610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing this impossible values: clients can not exist before their registration.\n",
    "\n",
    "df = df[~(df['CustomerSince']>'2018-12-31 23:59:59')]\n",
    "print('The total amount of impossible values is:', df[df['CustomerSince']>'2018-12-31 23:59:59'].shape[0])\n",
    "print('Percentage of data kept after removing this impossible values is:', np.round(df.shape[0] / df_original.shape[0], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac793bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing this impossible values: impossible to have an invoice from a client before their registration.\n",
    "\n",
    "df = df[~(df['InvoiceDateHour'] < df['CustomerSince'])]\n",
    "print('The total amount of impossible values is:', df[df['InvoiceDateHour'] < df['CustomerSince']].shape[0])\n",
    "print('Percentage of data kept after removing this impossible values is:', np.round(df.shape[0] / df_original.shape[0], 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f67ced2",
   "metadata": {},
   "source": [
    "- **3.1) Selecting data**\n",
    "- 3.1.2) Removing unnecessary features from Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8cf72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RELEVANCY\n",
    "# Dropping the following three features, which have high cardinality and does not provide any relevant informations: \n",
    "    # EmployeeID\n",
    "\n",
    "df.drop(columns=['EmployeeID'], inplace=True)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96890d2",
   "metadata": {},
   "source": [
    "- **3.1) Selecting data**\n",
    "- 3.1.3) Defining metric and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc099e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_features = df.select_dtypes(include=[np.number])#.columns.tolist()\n",
    "metric_features = [col for col in metric_features.columns if ('SR' not in col)]\n",
    "metric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280c9f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining all CATEGORICAL variables in the dataset. \n",
    "\n",
    "categorical_features = [column for column in df.columns if df[column].dtype == 'object']\n",
    "categorical_features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d5f97e",
   "metadata": {},
   "source": [
    "- **3.2) Cleaning data**\n",
    "- 3.2.1) Dealing with missing values and strange values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfcd586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'CustomerCity' distributed per 'Qty' and 'TotalAmount'.\n",
    "\n",
    "concat_df_3 = pd.concat([df.groupby([\"CustomerCity\"])[\"Qty\"].sum(), \n",
    "                       df.groupby([\"CustomerCity\"])[\"TotalAmount\"].sum()], axis=1)\n",
    "display(concat_df_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892e2a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we noticed above, there are some cities/places/district wrongly written, due to human mistake.\n",
    "# In this sense, we tried to merge the wrongly spelled ones to the correct ones.\n",
    "\n",
    "\n",
    "# LAKATAMEIA\n",
    "# We have confirmed how to spell this name correctly, replacing all those wrongly written citie/splaces/district\n",
    "# by the correct one. \n",
    "# Source:\n",
    "    # https://www.lakatamia.org.cy/en/home-en/\n",
    "df['CustomerCity'] = np.where(df['CustomerCity']=='Lakatame',\n",
    "                              'Lakatameia',df['CustomerCity'])\n",
    "df['CustomerCity'] = np.where(df['CustomerCity']=='Lakstameia',\n",
    "                              'Lakatameia',df['CustomerCity'])\n",
    "\n",
    "# STROBOLOS\n",
    "# We have confirmed how to spell this name correctly, replacing all those wrongly written cities/places/district \n",
    "# by the correct one. \n",
    "# Source:\n",
    "# https://www.strovolos.org.cy/\n",
    "df['CustomerCity'] = np.where(df['CustomerCity']=='Strobolo',\n",
    "                              'Strobolos',df['CustomerCity'])\n",
    "\n",
    "\n",
    "# EGKOMI\n",
    "# We have confirmed how to spell this name correctly, replacing all those wrongly written cities/places/district\n",
    "# by the correct one. \n",
    "# In this case, we checked that there are two different lines for the same city: EGKOMI.\n",
    "        # Then, we concluded it could only be due to fact that those values have different datatypes.\n",
    "# Source:\n",
    "    # https://ucm.org.cy/en/municipalities/nicosia-district/\n",
    "    \n",
    "df['CustomerCity'] = df['CustomerCity'].str.strip()\n",
    "df['CustomerCity'] = np.where(df['CustomerCity']=='Egkomi',\n",
    "                              'Egkomi',df['CustomerCity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b373b4c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# After cleaning up our data regarding cities/places, we visualized again our \n",
    "# 'CustomerCity' distributed per 'Qty' and 'TotalAmount'.\n",
    "\n",
    "concat_df_3 = pd.concat([df.groupby([\"CustomerCity\"])[\"Qty\"].sum(), \n",
    "                       df.groupby([\"CustomerCity\"])[\"TotalAmount\"].sum()], axis=1)\n",
    "display(concat_df_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d3bcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lowercase column using str.lower()\n",
    "df['CustomerCity'] = df['CustomerCity'].str.lower()\n",
    "\n",
    "# Capitalize first letter\n",
    "df['CustomerCity'] = df['CustomerCity'].str.capitalize()\n",
    "\n",
    "# Setting visual theme\n",
    "sns.set_theme(style = 'white', \n",
    "              palette = None)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(11,8))\n",
    "sns.countplot(data = df,\n",
    "              y = df['CustomerCity'],\n",
    "              color='lightsteelblue',\n",
    "              edgecolor='lightsteelblue',\n",
    "              order = df['CustomerCity'].value_counts().index\n",
    "              )\n",
    "# Layout\n",
    "plt.suptitle('CustomerCity', \n",
    "             fontsize = 20, \n",
    "             fontweight = 'bold')\n",
    "plt.legend(title='Absolute Values', \n",
    "           loc='lower right', \n",
    "           prop={'size': 14})\n",
    "plt.xlabel('Number of Customers', fontsize=14)\n",
    "plt.ylabel('City/Place/District', fontsize=14)\n",
    "plt.grid(axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17da8f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a visual notion of this chain locations restaurants.\n",
    "\n",
    "# Get the latitude and longitude of the map center\n",
    "center_lat, center_lon = 35.185566, 33.382275\n",
    "\n",
    "# Create a map centered on the coordinates above\n",
    "m = folium.Map(location=[center_lat, center_lon], zoom_start=8)\n",
    "\n",
    "# Create a Nominatim geolocator\n",
    "geolocator = Nominatim(user_agent=\"http\")\n",
    "\n",
    "# Create a rate limiter with a delay of 1 second between requests\n",
    "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)\n",
    "\n",
    "# Group the data by city and count the number of unique DocNumbers\n",
    "grouped = df.groupby('CustomerCity')['DocNumber'].nunique()\n",
    "\n",
    "# Create a marker for each city and add it to the map\n",
    "for city, count in grouped.items():\n",
    "    try:\n",
    "        # Get the coordinates of the city using OpenStreetMap Nominatim API\n",
    "        location = geocode(city)\n",
    "        if location:\n",
    "            lat, lon = location.latitude, location.longitude\n",
    "        else:\n",
    "            # Set the coordinates to (0, 0) if the location is not found\n",
    "            lat, lon = 0, 0\n",
    "\n",
    "        # Create a marker and add it to the map\n",
    "        folium.Marker([lat, lon], popup=f'{city}: {count}').add_to(m)\n",
    "    except (exc.GeocoderTimedOut, exc.GeocoderServiceError):\n",
    "        continue\n",
    "\n",
    "# DisplayÂ theÂ map\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1c2109",
   "metadata": {},
   "source": [
    "- **3.2) Cleaning data**\n",
    "- 3.2.3) Histograms Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448e139c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# HISTOGRAM (METRIC FEATURES)\n",
    "# Get a depper visual understanding on metric features through histograms.\n",
    "\n",
    "# Set layout\n",
    "sns.set_theme(style = 'white', palette = None)\n",
    "sns.set_style(\"ticks\",{'axes.grid' : False})\n",
    "\n",
    "n_col = 2\n",
    "n_row = math.ceil(df[metric_features].shape[1]/n_col)\n",
    "title = \"Metric Variables Histograms\"\n",
    "df[metric_features].hist(bins=10, \n",
    "                         figsize=(15, 13), \n",
    "                         layout=(n_row, n_col), \n",
    "                         xlabelsize=8, ylabelsize=8,\n",
    "                         color ='lightsteelblue'\n",
    "                         );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a58ea5",
   "metadata": {},
   "source": [
    "- **3.2) Cleaning data**\n",
    "- 3.2.4) Boxplots Analysis\n",
    "- 3.2.4.1) Manual Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a25ef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a depper visual understanding on METRIC FEATURES through Box Plots. \n",
    "# With this graph, we can have an even better understanding on existing outliers.\n",
    "\n",
    "# All Metric Variables' Box Plots in one figure.\n",
    "sns.set()\n",
    "sns.set_theme(style = 'white', \n",
    "              palette = None)\n",
    "\n",
    "# Prepare figure. Create indvidual axes where each box plot will be placed.\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 13))\n",
    "\n",
    "# Plot data.\n",
    "# Iterate across axes objects and associate each box plot (hint: use the ax argument):\n",
    "for ax, feat in zip(axes.flatten(), metric_features): \n",
    "    sns.boxplot(x=df[feat], ax=ax, color ='lightsteelblue')\n",
    "    # Customize\n",
    "    ax.grid(True)\n",
    "    \n",
    "# Layout\n",
    "title = \"Metric Variables Box Plots\" # Add a centered title to the figure\n",
    "fig.suptitle(title, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2470b2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Manual Removal (First method).\n",
    "    # Here, on outliers manual remotion, we decided to look both at the histograms, at the box plots and at the \n",
    "    # descriptive statistics resume.\n",
    "    # In our opinion, all these three were relevant instruments to filter each variable.\n",
    "        # So, based on these elements, we mostly adopted a 'try and error aproach'.\n",
    "\n",
    "filters_1 = (\n",
    "     (df['Pax']<=70) # Mostly, by looking at the boxplots and the histograms.\n",
    "     &\n",
    "     (df['Qty']<=(20)) # Mostly, by looking at the boxplots and the histograms.\n",
    "     &\n",
    "     (df['TotalAmount']<=(1000)) # Mostly, by looking at the boxplots and the histograms. \n",
    ")\n",
    "\n",
    "df_manual_removal = df[filters_1]\n",
    "\n",
    "print('Percentage of data kept after removing outliers in df_1:', \n",
    "      np.round(df_manual_removal.shape[0] / df_original.shape[0], 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1376dd14",
   "metadata": {},
   "source": [
    "- **3.2) Cleaning data**\n",
    "- 3.2.4) Boxplots Analysis\n",
    "- 3.2.4.2) IQR Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9477847",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T13:24:34.847043Z",
     "start_time": "2023-03-07T13:24:34.843015Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make another copy\n",
    "\n",
    "df_IQR_removal = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d6cf77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T13:24:34.937968Z",
     "start_time": "2023-03-07T13:24:34.848037Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Source: \n",
    "\n",
    "def check_IQR_outliers(data, criterion, mode='data'):\n",
    "    '''\n",
    "    Detects outliers by generating a decision range based on a criterion (threshold)\n",
    "    and locating values that don't fall within this range.\n",
    "    Can return the dataframe without the outliers or the percentage of remaining data\n",
    "    after outlier removal or the values considered as outliers for each column by\n",
    "    changing the parameter 'mode' (data as default)\n",
    "\n",
    "    Args:\n",
    "        data (pandas.core.frame.DataFrame) : set of data\n",
    "        col (pandas.core.series.Series) : column of the data to check outliers\n",
    "        criterion (int) : IQR multiplier, the higher, the larger the decision range\n",
    "        mode (str): 'data' to get dataframe without outliers 'perc' to check percentage\n",
    "                     or 'values' to check values\n",
    "\n",
    "    Returns:\n",
    "        Dataframe without\n",
    "        Print with percentage of remaining data after removing outliers or\n",
    "        index and values that fall outside the decision range (outliers)\n",
    "    '''\n",
    "    if mode == 'data':\n",
    "        Q1 = data.quantile(.25) # value of first quartile\n",
    "        Q3 = data.quantile(.75) # value of third quartile\n",
    "        IQR = Q3 - Q1 # interquartile range\n",
    "        lower_lim = Q1 - criterion * IQR # setting min limit\n",
    "        upper_lim = Q3 + criterion * IQR # setting max limit\n",
    "\n",
    "        outliers = []\n",
    "        for col in data.select_dtypes(np.number).columns:\n",
    "            llim = lower_lim[col]\n",
    "            ulim = upper_lim[col]\n",
    "            outliers.append(data[col].between(llim, ulim, inclusive='both'))\n",
    "\n",
    "        # np.all() tests whether all array elements along a given axis evaluate to True\n",
    "        outliers = data[np.all(outliers, 0)]\n",
    "\n",
    "        return outliers\n",
    "\n",
    "    elif mode == 'perc':\n",
    "        print('\\n\\033[1mPercentage of remaining data after outlier removal with IQR method:\\033[0m\\n')\n",
    "        Q1 = data.quantile(.25) # value of first quartile\n",
    "        Q3 = data.quantile(.75) # value of third quartile\n",
    "        IQR = Q3 - Q1 # interquartile range\n",
    "        lower_lim = Q1 - criterion * IQR # setting min limit\n",
    "        upper_lim = Q3 + criterion * IQR # setting max limit\n",
    "\n",
    "        outliers = []\n",
    "        for col in data.select_dtypes(np.number).columns:\n",
    "            #outliers.append(data[col].between(lower_lim[col], upper_lim[col], inclusive='both'))\n",
    "            llim = lower_lim[col]\n",
    "            ulim = upper_lim[col]\n",
    "            outliers.append(data[col].between(llim, ulim, inclusive='both'))\n",
    "\n",
    "        # np.all() tests whether all array elements along a given axis evaluate to True\n",
    "        outliers = data[np.all(outliers, axis=0)]\n",
    "\n",
    "        print(f'\\033[1m{round(len(outliers)/len(data)*100,3)} %\\033[0m of remaining data',\n",
    "              'after IQR method outlier removal, when criterion is', criterion)\n",
    "\n",
    "    else:\n",
    "        print('\\n\\n\\033[1mOutlier Values:\\033[0m\\n')\n",
    "        for col in data.select_dtypes(np.number).columns:\n",
    "            Q1 = data[col].quantile(.25) # value of first quartile\n",
    "            Q3 = data[col].quantile(.75) # value of third quartile\n",
    "            IQR = Q3 - Q1 # interquartile range\n",
    "            lower_lim = Q1 - criterion * IQR # setting min limit\n",
    "            upper_lim = Q3 + criterion * IQR # setting max limit\n",
    "            outliers = data.loc[(data[col] > upper_lim) | (data[col] < lower_lim)][col]\n",
    "            print(f'Outlier values in \\033[1m{col}\\033[0m:\\n')\n",
    "            for idx, val in outliers.items():\n",
    "                print(f'Value from Client {idx}: \\033[1m', round(val,2),'\\033[0m\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41208f75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T13:24:34.966476Z",
     "start_time": "2023-03-07T13:24:34.938879Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking percentage of remaining data\n",
    "check_IQR_outliers(data=df_IQR_removal, criterion=3, mode='perc')\n",
    "\n",
    "print('Percentage of data kept after removing outliers in df_1:', \n",
    "      np.round(df_IQR_removal.shape[0] / df_original.shape[0], 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e868816",
   "metadata": {},
   "source": [
    "**Key Takeawyays**\n",
    "- As we verified above, the IQR method, removes any value at all from our dataset.\n",
    "- Since under the manual approach, we have more accuracy on the data we are removing, we opted for that approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f854e223",
   "metadata": {},
   "source": [
    "- **3.2) Cleaning data**\n",
    "- 3.2.5) Correlations Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed6fa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The only two variables that are highly correlated are \"Pax\" and \"IsDelivery\".\n",
    "# However, as long as they are key for our analysis, the decided to do not drop any of them.\n",
    "\n",
    "def cor_heatmap(cor):\n",
    "    plt.figure(figsize=(20,20))\n",
    "    sns.heatmap(data = cor, \n",
    "                annot = True, \n",
    "                cmap = 'PuBu', \n",
    "                fmt='.1')\n",
    "    plt.title('Final Dataset Spearman Correlation Heatmap',fontsize = 34, fontweight = 'bold')\n",
    "    plt.show()\n",
    "    \n",
    "# Apply the correlation method to my dataset, usins spearman.\n",
    "cor_spearman = df[metric_features].corr(method= 'spearman')\n",
    "cor_heatmap(cor_spearman)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1fe2b8",
   "metadata": {},
   "source": [
    "- **3.3) Construct data**\n",
    "- 3.3.1) Creating new variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940cfca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating 'day' variable.\n",
    "df['day'] = df['InvoiceDateHour'].apply(lambda x:x.day)\n",
    "\n",
    "# Creating 'month' variable.\n",
    "df['month'] = df['InvoiceDateHour'].apply(lambda x:x.month)\n",
    "\n",
    "# Creating 'year' variable.\n",
    "df['year'] = df['InvoiceDateHour'].apply(lambda x:x.year)\n",
    "\n",
    "# Creating 'Hour_of_day' variable.\n",
    "df['Hour_of_day'] = df['InvoiceDateHour'].apply(lambda x:x.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6eec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing a graph for 'Invoice's Day' variable sorted descendently.\n",
    "\n",
    "# Setting visual theme\n",
    "sns.set_theme(style = 'white', \n",
    "              palette = None)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.countplot(data = df,\n",
    "              x = df['day'],\n",
    "              color='lightsteelblue',\n",
    "              edgecolor='lightsteelblue',\n",
    "              order = df['day'].value_counts().index\n",
    "              )\n",
    "\n",
    "# Layout\n",
    "plt.suptitle('Countplot for Days', \n",
    "             fontsize = 15, \n",
    "             fontweight = 'bold')\n",
    "plt.legend(title='', loc='lower right', prop={'size': 10})\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Frequency (In Absolute Value)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d589006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing a graph for 'Invoice's Month' variable sorted descendently.\n",
    "# Setting visual theme\n",
    "sns.set_theme(style = 'white', palette = None)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.countplot(data = df,\n",
    "              y = df['month'],\n",
    "              color='lightsteelblue',\n",
    "              edgecolor='lightsteelblue',\n",
    "              order = df['month'].value_counts().index\n",
    "              )\n",
    "\n",
    "# Layout\n",
    "plt.suptitle('Countplot for Month', fontsize = 15, fontweight = 'bold')\n",
    "plt.legend(title='', loc='lower right', prop={'size': 10})\n",
    "plt.xlabel('Frequency (In Absolute Value)')\n",
    "plt.ylabel('Month')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4273ffaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing a graph for 'Invoice's Hour_of_day' variable sorted descendently.\n",
    "# Setting visual theme\n",
    "sns.set_theme(style = 'white', palette = None)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.countplot(data = df,\n",
    "              x = df['Hour_of_day'],\n",
    "              color='lightsteelblue',\n",
    "              edgecolor='lightsteelblue',\n",
    "              order = df['Hour_of_day'].value_counts().index\n",
    "              )\n",
    "\n",
    "# Layout\n",
    "plt.suptitle('Countplot for Hour of the day', fontsize = 15, fontweight = 'bold')\n",
    "plt.legend(title='', loc='lower right', prop={'size': 10})\n",
    "plt.xlabel('Hour of the day')\n",
    "plt.ylabel('Frequency (In Absolute Value)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331d07f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding on which day of the week were the invoices issued.\n",
    "# Creating Invoice's Day of the Week attribute\n",
    "df['date_name'] = df['InvoiceDateHour'].dt.day_name()\n",
    "\n",
    "# # Visualizing the output\n",
    "# Setting visual theme\n",
    "sns.set_theme(style = 'white', palette = None)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.countplot(data = df,\n",
    "              y = df['date_name'],\n",
    "              color='lightsteelblue',\n",
    "              edgecolor='lightsteelblue',\n",
    "              order = df['date_name'].value_counts().index\n",
    "              )\n",
    "# Layout\n",
    "plt.suptitle('Countplot for Day of the Week', fontsize = 15, fontweight = 'bold')\n",
    "plt.legend(title='', loc='lower right', prop={'size': 10})\n",
    "plt.xlabel('Frequency (In Absolute Value)')\n",
    "plt.ylabel('Date of the Week')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac90f81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['InvoiceDateHour_datetime'] = pd.to_datetime(df['InvoiceDateHour'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "#df.drop(columns=['InvoiceDateHour'],inplace=True)\n",
    "\n",
    "\n",
    "# Understanding on which day of the week were the invoices issued.\n",
    "# Understanding whether the invoice was issued on weekend or not. Its a binary variable.\n",
    "# Understanding what what time of the day the invoice was issued.\n",
    "# Visualizing the output.\n",
    "\n",
    "# Source:\n",
    "    # https://stackoverflow.com/questions/62884585/select-weekends-and-weekdays-by-weekday-in-python\n",
    "\n",
    "df['Weekday_Invoice'] = df.InvoiceDateHour.dt.dayofweek\n",
    "df['Weekend_Invoice'] = np.where(df.InvoiceDateHour.dt.dayofweek.isin([5,6]), 1, 0)\n",
    "df['Hour_of_day'] = df['InvoiceDateHour'].dt.hour\n",
    "\n",
    "# Storing variables in a new dataframe called 'weekends'.\n",
    "weekends = df[df['Weekend_Invoice']==1]\n",
    "\n",
    "# Storing variables in a new dataframe called 'workdays'.\n",
    "workdays = df[df['Weekend_Invoice']!=1]\n",
    "display(weekends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2537174c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Getting a graphic visualization of the sales distribution by day of the week and by hour.\n",
    "\n",
    "# Conclusions:\n",
    "    # Costumers have similar conclusions throughout the week.\n",
    "    # Sunday shows more costumers during lunch time and less picks during dinner time.\n",
    "\n",
    "# Draw\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "g = joypy.joyplot(data=df, \n",
    "                  column='Hour_of_day', \n",
    "                  by='date_name',\n",
    "                  ax=ax,\n",
    "                  color='lightsteelblue') # set color parameter to lightsteelblue\n",
    "\n",
    "# Plot\n",
    "plt.title(\"Distribution by weekday and Hour_of_day\", \n",
    "          fontsize=15, \n",
    "          fontweight = 'bold')\n",
    "plt.xlabel(\"Hour_of_day\")\n",
    "plt.rc('axes', \n",
    "       labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0ff82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Totals sales by day of the week and delivery\n",
    "\n",
    "# Pivot table\n",
    "aggregated_df = df.pivot_table(values=['TotalAmount'], \n",
    "                               index='date_name',\n",
    "                               columns='IsDelivery',\n",
    "                               aggfunc='sum',\n",
    "                               fill_value=0)\n",
    "\n",
    "# Draw\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "g = aggregated_df.plot(kind='bar', \n",
    "                       stacked=True, \n",
    "                       ax=ax, \n",
    "                       color=['lightsteelblue', 'seagreen'])\n",
    "\n",
    "# Decoration\n",
    "fmt = \"{x:,.0f}\"\n",
    "tick = ticker.StrMethodFormatter(fmt)\n",
    "ax.yaxis.set_major_formatter(tick)\n",
    "sns.despine()\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "labels = ['Dinne - Inn Situation', 'Delivery Situation']  # updated labels\n",
    "ax.legend(handles=handles, \n",
    "          labels=labels, \n",
    "          loc='upper center', \n",
    "          ncol=4, \n",
    "          bbox_to_anchor=(0.5, 1.2),  # updated position\n",
    "          frameon=False)\n",
    "\n",
    "# Plot\n",
    "plt.title(\"Sales by weekday and type of sell\", fontsize=15, fontweight='bold')\n",
    "plt.xlabel(\"Weekday (including Weekends)\")\n",
    "plt.ylabel(\"Total sales amount\")\n",
    "plt.xticks(rotation=0, ha='center')\n",
    "plt.rc('axes', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479f3fc2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We aimed to understand at which time of the week and at which time of the day, people use to come\n",
    "# to the restaurants.\n",
    "\n",
    "# Conclusions:\n",
    "    # The most busy days of the week are:\n",
    "        # Friday\n",
    "        # Saturday\n",
    "    # Sunday presents usually has some busy lunch hours. \n",
    "        \n",
    "\n",
    "grouped_df = df.groupby([\"date_name\", \"Hour_of_day\"])[\"TotalAmount\"].aggregate(\"count\").reset_index()\n",
    "grouped_df = grouped_df.pivot('date_name', 'Hour_of_day', 'TotalAmount')\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.heatmap(grouped_df, cmap='PuBu')\n",
    "plt.title(\"Frequency of Day of week Vs Hour of the day\", fontsize=15, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7162e16",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating Invoice's Day of the Week attribute\n",
    "'''\n",
    "df.loc[(df['month']>= 1) &(df['month'] <=2), 'season'] = \"Winter\"\n",
    "df.loc[(df['month']>= 3) &(df['month'] <=4), 'season'] = \"Spring\"\n",
    "df.loc[(df['month']>= 5) &(df['month'] <=6), 'season'] = \"Summer\"\n",
    "df.loc[(df['month']>= 7) &(df['month'] <=8), 'season'] = \"Monsoon\"\n",
    "df.loc[(df['month']>= 9) &(df['month'] <=10), 'season'] = \"Autumn\"\n",
    "df.loc[(df['month']>= 11) &(df['month'] <=12), 'season'] = \"Winter\"\n",
    "'''\n",
    "\n",
    "df.loc[(df['month']>= 1) &(df['month'] <=2), 'season'] = \"Winter\"\n",
    "df.loc[(df['month']>= 3) &(df['month'] <=5), 'season'] = \"Spring\"\n",
    "df.loc[(df['month']>= 6) &(df['month'] <=8), 'season'] = \"Summer\"\n",
    "df.loc[(df['month']>= 9) &(df['month'] <=11), 'season'] = \"Autumn\"\n",
    "df.loc[(df['month']> 11) &(df['month'] <=12), 'season'] = \"Winter\"\n",
    "\n",
    "# Drawing a graph for Invoice's Day of the Week attribute\n",
    "# Setting visual theme\n",
    "sns.set_theme(style = 'white', palette = None)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "sns.countplot(data = df,\n",
    "              y = df['season'],\n",
    "              color='lightsteelblue',\n",
    "              edgecolor='lightsteelblue',\n",
    "              order = df['season'].value_counts().index\n",
    "              )\n",
    "# Layout\n",
    "plt.suptitle('Countplot for Seasons', fontsize = 15, fontweight = 'bold')\n",
    "plt.legend(title='', loc='lower right', prop={'size': 10})\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Season\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dda2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing a graph for Total Amount per Day\n",
    "# Setting visual theme\n",
    "sns.set_theme(style = 'white', palette = None)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "sns.lineplot(x=\"day\", \n",
    "             y=\"TotalAmount\",\n",
    "             hue=\"IsDelivery\",\n",
    "             palette = [\"#aec7e8\", '#2ca02c'], \n",
    "             data=df)\n",
    "# set the title and labels\n",
    "plt.suptitle('TotalAmount per day - Delivery x Restaurant', fontsize=15, fontweight='bold', y=1.05)\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Total Amount')\n",
    "#plt.legend(['Restaurant', 'Delivery'], loc='lower right', prop={'size': 10})\n",
    "plt.grid()\n",
    "\n",
    "# rotate and move the y-axis label\n",
    "plt.ylabel('Total Amount', ha='right',fontsize=16, labelpad = 20)\n",
    "ax = plt.gca()\n",
    "ax.yaxis.set_label_coords(-0.05, 0.5)\n",
    "\n",
    "#plt.title('Frequency of Customers by City', fontsize=20, fontweight='bold')\n",
    "plt.xlabel('Per Day', fontsize=16)\n",
    "plt.ylabel('Total Amount', fontsize=16)\n",
    "\n",
    "# display theÂ plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b48dd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating National Holidays attribute\n",
    "# Based on https://www.timeanddate.com/holidays/cyprus/2018\n",
    "\n",
    "df['Date'] = df.InvoiceDateHour.apply(lambda x: x.date())\n",
    "\n",
    "NationalHolidays = ['01-01-2018','06-01-2018','19-02-2018','25-03-2018','01-04-2018',\n",
    "                    '06-04-2018','07-04-2018', '08-04-2018','09-04-2018','01-05-2018',\n",
    "                    '28-05-2018','15-08-2018','01-10-2018','28-10-2018', '24-12-2018',\n",
    "                    '25-12-2018','26-12-2018','31-12-2018']\n",
    "\n",
    "holidays_date = pd.to_datetime(NationalHolidays, format='%d-%m-%Y')\n",
    "holiday_dates = holidays_date.strftime('%Y-%m-%d').tolist()\n",
    "\n",
    "df['Holiday'] = df.Date.apply(lambda x: 1 if str(x) in holiday_dates else 0)\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be25dfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drawing a graph for Invoice's Day of the Week attribute\n",
    "# Setting visual theme\n",
    "sns.set_theme(style = 'white', palette = None)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "sns.countplot(data = df,\n",
    "              y = df['Holiday'],\n",
    "              color='lightsteelblue',\n",
    "              edgecolor='lightsteelblue',\n",
    "              order = df['Holiday'].value_counts().index\n",
    "              )\n",
    "# Layout\n",
    "plt.suptitle('Countplot for Holidays', fontsize = 15, fontweight = 'bold')\n",
    "plt.legend(title='', loc='lower right', prop={'size': 10})\n",
    "plt.xlabel('Frequency (In Absolute Values)')\n",
    "plt.ylabel('Holiday')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0835438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'three most profitable' holidays are:\n",
    "    # 24.12.2018, Christmas Eve\n",
    "    # 31.12.2018, New Year's Eve\n",
    "    # 06.01.2018, Epiphany\n",
    "# This three holidays are separated for just 13 days. We suggest a special campaign during these two weeks. \n",
    "\n",
    "# Three 'two less profitable' holidays are:\n",
    "    # 01.05.2018, Labour Day/May Day\n",
    "    # 28.05.2018, Orthodox Pentecost Monday\n",
    "\n",
    "# Group the data by holiday status and date and calculate the total revenue for each group\n",
    "revenue_by_holiday = df[df['Holiday'] == 1].groupby(['Holiday', 'Date'])['TotalAmount'].sum()\n",
    "\n",
    "# Layout\n",
    "# Plot\n",
    "ax = revenue_by_holiday.plot.bar(figsize=(10, 8),\n",
    "                                color = ['lightsteelblue'])\n",
    "ax.set_title('Total Revenue by Holiday')\n",
    "ax.set_xlabel('Holiday')\n",
    "ax.set_ylabel('TotalAmount')\n",
    "plt\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5d0b17",
   "metadata": {},
   "source": [
    "# **4. Modeling**\n",
    "\n",
    "\n",
    "- **4.1) Association Rules**\n",
    "- 4.1.1) Creating Pivot tables for apriori\n",
    "- **4.2) Modelling: By Product**\n",
    "- 4.2.2) Overall DataSet: Defining and Exploring some metrics as support, confidence and lift.\n",
    "- 4.2.2) Key Takeaways\n",
    "- **4.3) Modelling: Dinne-Inns**\n",
    "- 4.3.1) Overall DataSet: Defining and Exploring some metrics as support, confidence and lift.\n",
    "- 4.3.2) Key Takeaways\n",
    "- **4.4) Modelling: Deliveries**\n",
    "- 4.4.1) Overall DataSet: Defining and Exploring some metrics as support, confidence and lift.\n",
    "- 4.4.2) Key Takeaways\n",
    "- **4.5) Modelling: Dinne-Inns Excluding Water**\n",
    "- 4.5.1) Overall DataSet: Defining and Exploring some metrics as support, confidence and lift.\n",
    "- 4.5.2) Key Takeaways\n",
    "- **4.6) Modelling: By Family**\n",
    "- 4.6.1) Overall DataSet: Defining and Exploring some metrics as support, confidence and lift.\n",
    "- 4.6.2) Key Takeaways\n",
    "- **4.7) Menus Suggestions**\n",
    "- 4.7.1) Menus for Dinne-Inns\n",
    "- 4.7.2) Menus for Deliveries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b59a4d3",
   "metadata": {},
   "source": [
    "- **4.1) Association Rules**\n",
    "- 4.1.1) Creating Pivot tables for apriori"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57debca",
   "metadata": {},
   "source": [
    "- **ASSUMPTIONS**\n",
    "<br>\n",
    "<br>\n",
    "- **First Approach:** Firstly, we tried to understand the main conclusions analyzing both by product and by product family, without including any filter in our selection.\n",
    "<br>\n",
    "- **Second Approach:** As we saw previously throughout our report, since there are very different behaviours patterns between Dinne-Inns and Deliveries situations, we decided to create two different datasets and analyzing them separately. Additionally, in the Deliveries dataset, we decided to not include two products: Delivery Charge and Tsanta, both products related with the delivery process (not a meal itself), so that we get a cleaner visualization of the association rules.\n",
    "<br>\n",
    "- **Third Approach (Dinne - Inns):** after the first iteration, we noticed that the product Mineral water 1.5lt appeared very frequently in the association rules. In this sense, we decided to create a new dataset, derived from dinne-inn, but without including Mineral water 1.5lt, in order to understand deeply the behaviour patterns of the costumers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6022505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we are creating the pivot tables to apply the apriori.\n",
    "\n",
    "# 1. By Product, no restriction\n",
    "df_pt = pd.pivot_table(df[['DocNumber','ProductDesignation']],\n",
    "                    index='DocNumber',                                 # Each line represents a document (invoice).\n",
    "                    columns='ProductDesignation',                      # Each column will represents a feature.\n",
    "                    aggfunc=lambda x: 1 if len(x)>0 else 0).fillna(0)  # The value should be 1 if the product is present in the document, otherwise 0.\n",
    "\n",
    "# 2. By Product, delivery.\n",
    "df_pt_delivery = pd.pivot_table(df[(df.IsDelivery == 1)&(df.ProductDesignation != 'Tsanta')][[\"DocNumber\",\"ProductDesignation\"]], \n",
    "                         index='DocNumber', \n",
    "                         columns='ProductDesignation', \n",
    "                         aggfunc=lambda x: 1 if len(x)>0 else 0).fillna(0).drop(\"Delivery charge\",axis=1) \n",
    "\n",
    "\n",
    "# 3. By Product, dinne_inn.\n",
    "df_pt_dinne_in = pd.pivot_table(df[df.IsDelivery == 0][[\"DocNumber\",\"ProductDesignation\"]], \n",
    "                                index='DocNumber', \n",
    "                                columns='ProductDesignation', \n",
    "                                aggfunc=lambda x: 1 if len(x)>0 else 0).fillna(0) \n",
    "\n",
    "\n",
    "# 4. Dinne-Inns - excluding Water, since its highly unbalanced.\n",
    "df_pt_dinne_in_no_water = pd.pivot_table(df[(df.IsDelivery == 0)&(df.ProductDesignation != 'Mineral water 1.5lt')][[\"DocNumber\",\"ProductDesignation\"]], \n",
    "                            index='DocNumber', \n",
    "                            columns='ProductDesignation', \n",
    "                            aggfunc=lambda x: 1 if len(x)>0 else 0).fillna(0) \n",
    "\n",
    "\n",
    "# 5. By Family, no restriction.\n",
    "df_pt_family = pd.pivot_table(df[['DocNumber','ProductFamily']],\n",
    "                    index='DocNumber',                                  # Each line represents a document (invoice).\n",
    "                    columns='ProductFamily',                            # Each column will represents a feature.\n",
    "                    aggfunc=lambda x: 1 if len(x)>0 else 0).fillna(0)   # The value should be 1 if the product is present in the document, otherwise 0\n",
    "\n",
    "\n",
    "# Visualizing the table 1, by product with no restriction.\n",
    "print(\"If we do not define any threshold, we have the following amount of association rules:\", df_pt.shape)\n",
    "df_pt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ede1df",
   "metadata": {},
   "source": [
    "- **4.2) Modelling: Overall DataSet**\n",
    "- 4.2.2) Overall DataSet: Defining and Exploring some metrics as support, confidence and lift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12d9a35",
   "metadata": {},
   "source": [
    "- **Itemset:** \"list of all the items in the antecedent and the consequent\".\n",
    "- **Support:** \"the fraction of the total number of transactions in which the itemset occurs (%)\". \n",
    "- **Antecedent Support:** represents the frequency of the antecedent itemset in the dataset (%).\n",
    "- **Consequent Support:** represents the frequency of the consequent itemset in the dataset (%).\n",
    "- **Confidence:** \"the conditional probability of occurrence of consequent given the antecedent (%)\".\n",
    "- **Lift:** \"the conditional probability of occurrence of consequent given the antecedent\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b63c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we aim to apply the APRIORI algorithm.\n",
    "# The 'min_support' parameter, measn that this rules are supported in more than 5% of the transactions.\n",
    "# Source:\n",
    "    # http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/\n",
    "\n",
    "# We can define the itemset as \"the list of all the items in the antecedent and the consequent\".\n",
    "# Source:\n",
    "    # https://towardsdatascience.com/association-rules-2-aa9a77241654\n",
    "    # https://www.geeksforgeeks.org/association-rule/\n",
    "\n",
    "frequent_itemsets = apriori(df_pt, min_support=0.05, use_colnames=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5642334",
   "metadata": {},
   "source": [
    "**Defining a threshold of 10% for support** <br>\n",
    "**Defining a threshold of 50% for confidence** <br>\n",
    "**Defining a threshold of 1.5 for lift** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae726557",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here, we aim to generate the association rules - by support.\n",
    "# According to the same source, the support is \"the fraction of the total number of transactions in which the itemset occurs\".\n",
    "# Source:\n",
    "    # https://towardsdatascience.com/association-rules-2-aa9a77241654\n",
    "    # https://www.geeksforgeeks.org/association-rule/\n",
    "    # https://www.ibm.com/docs/en/db2/9.7?topic=associations-support-in-association-rule\n",
    "rules_support = association_rules(frequent_itemsets, metric=\"support\", min_threshold=0.1)\n",
    "rules_support.sort_values(by='support', ascending=False, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Here, we aim to generate the association rules - by confidence.\n",
    "# According to the same source, the confidence is \"the conditional probability of occurrence of \n",
    "# consequent given the antecedent\".\n",
    "# Source:\n",
    "    # https://towardsdatascience.com/association-rules-2-aa9a77241654\n",
    "    # https://www.geeksforgeeks.org/association-rule/\n",
    "rules_confidence = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.50)\n",
    "rules_confidence.sort_values(by='confidence', ascending=False, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Here, we aim to generate the association rules - by lift.\n",
    "# According to the same source, the lift is \"the conditional probability of occurrence of \n",
    "# consequent given the antecedent\".\n",
    "# Source:\n",
    "    # https://towardsdatascience.com/association-rules-2-aa9a77241654\n",
    "    # https://www.geeksforgeeks.org/association-rule/\n",
    "    # https://www.ibm.com/docs/en/db2/11.1?topic=SSEPGG_11.1.0/com.ibm.im.model.doc/c_lift_in_an_association_rule.htm\n",
    "rules_lift = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.5)\n",
    "rules_lift.sort_values(by='lift', ascending=False, inplace=True)\n",
    "\n",
    "# Visualizing the lift values sorted descendently.\n",
    "rules_lift.sort_values(by=['lift'], ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bd0bb3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Scatter plot of these rules.\n",
    "# We cand understand below that normally, there is a tendency for a positive correlation between\n",
    "# the metrics 'support' and 'confidence', although without a very high value in absolute terms.\n",
    "# The size\n",
    "\n",
    "\n",
    "# Draw\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "g = sns.scatterplot(data=rules_support, \n",
    "                    x=\"support\", \n",
    "                    y=\"confidence\", \n",
    "                    size=\"lift\", \n",
    "                    sizes=(30, 250), \n",
    "                    alpha=0.8)\n",
    "\n",
    "# Plot\n",
    "sns.despine()\n",
    "plt.title(\"Rules with a minimum support threshold of 10% (Lift represeted by the size)\", fontsize=12)\n",
    "plt.xlabel(\"Support level\")\n",
    "plt.ylabel(\"Confidence level\")\n",
    "plt.rc('axes', labelsize=12)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles, labels=labels, loc='upper center', \n",
    "          ncol=6, bbox_to_anchor=(0.45, 1.03), frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5db55a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Bubble plot of the rules.\n",
    "\n",
    "# The plot displays the relationship between the support and confidence levels of different rules in a dataset, \n",
    "# where each point represents a rule. \n",
    "# In this plot, the size of each point (ball) represents the lift, which is a measure of the \n",
    "# strength of the association between the antecedent and consequent of a rule. \n",
    "# Points with a higher lift will have a bigger size, and points with a lower lift will have a smaller size.\n",
    "    # Therefore, bigger balls on the plot indicate that the corresponding rules have a higher lift, \n",
    "    # which means a stronger association between the antecedent and consequent of the rule.\n",
    "\n",
    "# Replace frozen sets with strings\n",
    "rules_confidence['antecedents_'] = rules_confidence['antecedents'].apply(lambda a: ','.join(list(a)))\n",
    "rules_confidence['consequents_'] = rules_confidence['consequents'].apply(lambda a: ','.join(list(a)))\n",
    "\n",
    "# Draw\n",
    "fig, ax = plt.subplots(figsize=(14,14))\n",
    "ax= plt.scatter(data=rules_confidence, \n",
    "                x='consequents_', \n",
    "                y='antecedents_', \n",
    "                s = rules_confidence['lift']*20, \n",
    "                edgecolors = \"blue\", \n",
    "                c = \"white\", \n",
    "                zorder = 2)\n",
    "\n",
    "# Plot\n",
    "nRules=rules_confidence.shape[0]\n",
    "plt.title(f\"Grouped matrix of the {nRules} rules\", fontsize=12)\n",
    "plt.xlabel(\"Consequents (RHS)\")\n",
    "plt.ylabel(\"Antecedents (LHS)\")\n",
    "plt.grid(ls = \"--\", zorder = 1)\n",
    "fig.autofmt_xdate()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdeacb8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# General summary for rules lift threshold defined.\n",
    "\n",
    "rules_lift.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918ddf48",
   "metadata": {},
   "source": [
    "**Exploring the most frequent items**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b931dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measuring the quantity of products per set.\n",
    "\n",
    "frequent_itemsets['products_per_set'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "\n",
    "\n",
    "# Checking the results for a 'products_per_set' = 2 and a Support >= 0.1.\n",
    "# Reminding, a support repreents the percentage of transactions in the entire dataset that contain a particular itemset. \n",
    "# In this case, we aim to filter all the itemsets that represents at least 10% of the total transactions.\n",
    "\n",
    "frequent_itemsets[(frequent_itemsets['products_per_set'] == 2) & (frequent_itemsets['support'] >= 0.17)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a33f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Itemsets with both high 'Confidence' and high 'Lift'.\n",
    "\n",
    "rules_confidence[(rules_confidence['lift'] >= 3) & (rules_confidence['confidence'] >= 0.7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c39e7a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# First draft.\n",
    "# Plot a basic network graph of the top 20 confidence rules\n",
    "\n",
    "# Create a copy of the rules and transform the frozensets to strings\n",
    "rulesToPlot = rules_confidence.copy(deep=True)\n",
    "rulesToPlot['LHS'] = [','.join(list(x)) for x in rulesToPlot['antecedents']]\n",
    "rulesToPlot['RHS'] = [','.join(list(x)) for x in rulesToPlot['consequents']]\n",
    "\n",
    "# Remove duplicate if reversed rules\n",
    "rulesToPlot['sortedRow'] = [sorted([a,b]) for a,b in zip(rulesToPlot.LHS, rulesToPlot.RHS)]\n",
    "rulesToPlot['sortedRow'] = rulesToPlot['sortedRow'].astype(str)\n",
    "rulesToPlot.drop_duplicates(subset=['sortedRow'], inplace=True)\n",
    "\n",
    "# Plot Graphically\n",
    "rulesToPlot=rulesToPlot[:20]\n",
    "fig = plt.figure(figsize=(25, 25)) \n",
    "G = nx.from_pandas_edgelist(rulesToPlot, 'LHS', 'RHS')\n",
    "np.random.seed(1000)\n",
    "nx.draw(G, with_labels=True, \n",
    "        node_size=30, \n",
    "        node_color=\"lightsteelblue\", \n",
    "        font_size = 18,\n",
    "        pos=nx.spring_layout(G))\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d86882",
   "metadata": {},
   "source": [
    "- **4.2) Modelling: By Product**\n",
    "- 4.2.2) Key Takeaways <br>\n",
    "<br>\n",
    "- **Most frequent consequents:** Egg Fried Rice, Mineral Water 1.5LT, Delivery Charge and Noodles with meat.\n",
    "- All the dots that are closer to each of these \"centers\", are the most usual antecedents.\n",
    "- The closer is the point, the higher is the confidence level.\n",
    "- We still verify that the product Tsanta is closely linked with Delivery Charge. Both most not be taken into consideration. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dccabf",
   "metadata": {},
   "source": [
    "- **4.3) Modelling: Dinne-Inns**\n",
    "- 4.3.1) Overall DataSet: Defining and Exploring some metrics as support, confidence and lift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2009fa",
   "metadata": {},
   "source": [
    "**Defining a threshold of 10% for support** <br>\n",
    "**Defining a threshold of 50% for confidence** <br>\n",
    "**Defining a threshold of 0 for lift (to analyze possible substitute products)** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db23727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DINNE INNS\n",
    "frequent_itemsets_dinne_in = apriori(df_pt_dinne_in, min_support=0.05, use_colnames=True) \n",
    "\n",
    "# Defining Support\n",
    "rules_support_dinne_in = association_rules(frequent_itemsets_dinne_in, metric=\"support\", min_threshold=0.10) \n",
    "rules_support_dinne_in.sort_values(by='support', ascending=False, inplace=True) \n",
    "print(rules_support_dinne_in.head(3))\n",
    "\n",
    "# Defining Confidence\n",
    "rules_confidence_dinne_in = association_rules(frequent_itemsets_dinne_in, metric=\"confidence\", min_threshold=0.50) \n",
    "rules_confidence_dinne_in.sort_values(by='confidence', ascending=False, inplace=True) \n",
    "print(rules_confidence_dinne_in.head(3))\n",
    " \n",
    "# Defining Lift\n",
    "rules_lift_dinne_in = association_rules(frequent_itemsets_dinne_in, metric=\"lift\", min_threshold=0)  \n",
    "rules_lift_dinne_in.sort_values(by='lift', ascending=False, inplace=True) \n",
    "rules_lift_dinne_in.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76f7d3a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# First draft.\n",
    "# Plot a basic network graph of the top 20 confidence rules\n",
    "\n",
    "# Create a copy of the rules and transform the frozensets to strings\n",
    "rulesToPlot_dinne_in = rules_confidence_dinne_in.copy(deep=True)\n",
    "rulesToPlot_dinne_in['LHS'] = [','.join(list(x)) for x in rulesToPlot_dinne_in['antecedents']]\n",
    "rulesToPlot_dinne_in['RHS'] = [','.join(list(x)) for x in rulesToPlot_dinne_in['consequents']]\n",
    "\n",
    "# Remove duplicate if reversed rules\n",
    "rulesToPlot_dinne_in['sortedRow'] = [sorted([a,b]) for a,b in zip(rulesToPlot_dinne_in.LHS, rulesToPlot_dinne_in.RHS)]\n",
    "rulesToPlot_dinne_in['sortedRow'] = rulesToPlot_dinne_in['sortedRow'].astype(str)\n",
    "rulesToPlot_dinne_in.drop_duplicates(subset=['sortedRow'], inplace=True)\n",
    "\n",
    "# Plot Graphically\n",
    "rulesToPlot_dinne_in=rulesToPlot_dinne_in[:20]\n",
    "fig = plt.figure(figsize=(22, 22)) \n",
    "G = nx.from_pandas_edgelist(rulesToPlot_dinne_in, 'LHS', 'RHS')\n",
    "np.random.seed(1000)\n",
    "nx.draw(G, with_labels=True, \n",
    "        node_size=30, \n",
    "        node_color=\"red\", \n",
    "        font_size = 18,\n",
    "        pos=nx.spring_layout(G))\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb81484",
   "metadata": {},
   "source": [
    "- **4.3) Modelling: Dinne-Inns**\n",
    "- 4.3.2) Key Takeaways <br>\n",
    "<br>\n",
    "- **Most frequent consequents:** Mineral Water 1.5LT and Noodles with meat.\n",
    "- All the dots that are closer to each of these \"centers\", are the most usual antecedents.\n",
    "- The closer is the point, the higher is the confidence level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0273654b",
   "metadata": {},
   "source": [
    "- **4.4) Modelling: Deliveries**\n",
    "- 4.4.1) Overall DataSet: Defining and Exploring some metrics as support, confidence and lift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a5005c",
   "metadata": {},
   "source": [
    "**Defining a threshold of 10% for support** <br>\n",
    "**Defining a threshold of 50% for confidence** <br>\n",
    "**Defining a threshold of 0 for lift (to analyze possible substitute products)** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2db9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DElIVERIES\n",
    "frequent_itemsets_delivery = apriori(df_pt_delivery, min_support=0.05, use_colnames=True) \n",
    "\n",
    "# Defining Support\n",
    "rules_support_delivery = association_rules(frequent_itemsets_delivery, metric=\"support\", min_threshold=0.10) \n",
    "rules_support_delivery.sort_values(by='support', ascending=False, inplace=True) \n",
    "print(rules_support_delivery.head(3))\n",
    "\n",
    "# Defining Confidence\n",
    "rules_confidence_delivery = association_rules(frequent_itemsets_delivery, metric=\"confidence\", min_threshold=0.50) \n",
    "rules_confidence_delivery.sort_values(by='confidence', ascending=False, inplace=True) \n",
    "print(rules_confidence_delivery.head(3))\n",
    " \n",
    "# Defining Lift\n",
    "rules_lift_delivery = association_rules(frequent_itemsets_delivery, metric=\"lift\", min_threshold=0)  \n",
    "rules_lift_delivery.sort_values(by='lift', ascending=False, inplace=True) \n",
    "rules_lift_delivery.head(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd2609d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First draft.\n",
    "# Plot a basic network graph of the top 20 confidence rules\n",
    "\n",
    "# Create a copy of the rules and transform the frozensets to strings\n",
    "rulesToPlot_delivery = rules_confidence_delivery.copy(deep=True)\n",
    "rulesToPlot_delivery['LHS'] = [','.join(list(x)) for x in rulesToPlot_delivery['antecedents']]\n",
    "rulesToPlot_delivery['RHS'] = [','.join(list(x)) for x in rulesToPlot_delivery['consequents']]\n",
    "\n",
    "# Remove duplicate if reversed rules\n",
    "rulesToPlot_delivery['sortedRow'] = [sorted([a,b]) for a,b in zip(rulesToPlot_delivery.LHS, rulesToPlot_delivery.RHS)]\n",
    "rulesToPlot_delivery['sortedRow'] = rulesToPlot_delivery['sortedRow'].astype(str)\n",
    "rulesToPlot_delivery.drop_duplicates(subset=['sortedRow'], inplace=True)\n",
    "\n",
    "# Plot Graphically\n",
    "rulesToPlot_delivery=rulesToPlot_delivery[:15]\n",
    "fig = plt.figure(figsize=(17, 17)) \n",
    "G = nx.from_pandas_edgelist(rulesToPlot_delivery, 'LHS', 'RHS')\n",
    "np.random.seed(1000)\n",
    "nx.draw(G, with_labels=True, \n",
    "        node_size=30, \n",
    "        node_color=\"red\", \n",
    "        font_size = 18,\n",
    "        pos=nx.spring_layout(G))\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8257a2",
   "metadata": {},
   "source": [
    "- **4.4) Modelling: Deliveries**\n",
    "- 4.4.2) Key Takeaways <br>\n",
    "<br>\n",
    "- **Most frequent consequents:** Sweet Sour Chicken and Egg Fried Rice.\n",
    "- All the dots that are closer to each of these \"centers\", are the most usual antecedents.\n",
    "- The closer is the point, the higher is the confidence level.\n",
    "- There are three smaller groups, not linked with the main ones above referred which means that this products are not usually ordered along with the remaning clusters of products."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fdfcaf",
   "metadata": {},
   "source": [
    "- **4.5) Modelling: Dinne-Inns Excluding Water**\n",
    "- 4.5.1) Overall DataSet: Defining and Exploring some metrics as support, confidence and lift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b719cc0",
   "metadata": {},
   "source": [
    "**Defining a threshold of 10% for support** <br>\n",
    "**Defining a threshold of 50% for confidence** <br>\n",
    "**Defining a threshold of 0 for lift (to analyze possible substitute products)** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebac96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DINNE INNS EXCLUDING WATER\n",
    "frequent_itemsets_dinne_in_no_water = apriori(df_pt_dinne_in_no_water, min_support=0.05, use_colnames=True) \n",
    "\n",
    "# Defining Support\n",
    "rules_support_dinne_in_no_water = association_rules(frequent_itemsets_dinne_in_no_water, metric=\"support\", min_threshold=0.10) \n",
    "rules_support_dinne_in_no_water.sort_values(by='support', ascending=False, inplace=True) \n",
    "print(rules_support_dinne_in_no_water.head(3))\n",
    "\n",
    "# Defining Confidence\n",
    "rules_confidence_dinne_in_no_water = association_rules(frequent_itemsets_dinne_in_no_water, metric=\"confidence\", min_threshold=0.50) \n",
    "rules_confidence_dinne_in_no_water.sort_values(by='confidence', ascending=False, inplace=True) \n",
    "print(rules_confidence_dinne_in_no_water.head(3))\n",
    " \n",
    "# Defining Lift\n",
    "rules_lift_dinne_in_no_water = association_rules(frequent_itemsets_dinne_in_no_water, metric=\"lift\", min_threshold=0)  \n",
    "rules_lift_dinne_in_no_water.sort_values(by='lift', ascending=False, inplace=True) \n",
    "rules_lift_dinne_in_no_water.head(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bf36e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First draft.\n",
    "# Plot a basic network graph of the top 15 confidence rules\n",
    "\n",
    "# Create a copy of the rules and transform the frozensets to strings\n",
    "rulesToPlot_dinne_in_no_water = rules_confidence_dinne_in_no_water.copy(deep=True)\n",
    "rulesToPlot_dinne_in_no_water['LHS'] = [','.join(list(x)) for x in rulesToPlot_dinne_in_no_water['antecedents']]\n",
    "rulesToPlot_dinne_in_no_water['RHS'] = [','.join(list(x)) for x in rulesToPlot_dinne_in_no_water['consequents']]\n",
    "\n",
    "# Remove duplicate if reversed rules\n",
    "rulesToPlot_dinne_in_no_water['sortedRow'] = [sorted([a,b]) for a,b in zip(rulesToPlot_dinne_in_no_water.LHS, rulesToPlot_dinne_in_no_water.RHS)]\n",
    "rulesToPlot_dinne_in_no_water['sortedRow'] = rulesToPlot_dinne_in_no_water['sortedRow'].astype(str)\n",
    "rulesToPlot_dinne_in_no_water.drop_duplicates(subset=['sortedRow'], inplace=True)\n",
    "\n",
    "# Plot Graphically\n",
    "rulesToPlot_dinne_in_no_water=rulesToPlot_dinne_in_no_water[:15]\n",
    "fig = plt.figure(figsize=(17, 17)) \n",
    "G = nx.from_pandas_edgelist(rulesToPlot_dinne_in_no_water, 'LHS', 'RHS')\n",
    "np.random.seed(1000)\n",
    "nx.draw(G, with_labels=True, \n",
    "        node_size=30, \n",
    "        node_color=\"red\", \n",
    "        pos=nx.spring_layout(G))\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1765b6c",
   "metadata": {},
   "source": [
    "- **4.5) Modelling: Dinne-Inns Excluding Water**\n",
    "- 4.5.2) Key Takeaways<br>\n",
    "<br>\n",
    "- **Most frequent consequents:** Egg Fried Rice, Spring Roll and No Meat.\n",
    "- All the dots that are closer to each of these \"centers\", are the most usual antecedents.\n",
    "- The closer is the point, the higher is the confidence level.\n",
    "- We can also observe that Toffe Banana Complementary and Beef BBS are an antececedent to both the consequence above mentioned.\n",
    "- For example when compared with the dataset **WITH WATER**, we can understand here the relevance of Egg fried rice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1270a7f",
   "metadata": {},
   "source": [
    "- **4.6) Modelling: By Family**\n",
    "- 4.6.1) Overall DataSet: Defining and Exploring some metrics as support, confidence and lift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c9d5c5",
   "metadata": {},
   "source": [
    "**Defining a threshold of 20% for support (since the granularity is bigger, the support is also higher, so we also increased the support threshold)** <br>\n",
    "**Defining a threshold of 50% for confidence** <br>\n",
    "**Defining a threshold of 0 for lift (to analyze possible substitute products)** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33f3948",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DINNE INNS EXCLUDING WATER\n",
    "frequent_itemsets_family = apriori(df_pt_family, min_support=0.05, use_colnames=True) \n",
    "\n",
    "# Defining Support\n",
    "rules_support_family = association_rules(frequent_itemsets_family, metric=\"support\", min_threshold=0.20) \n",
    "rules_support_family.sort_values(by='support', ascending=False, inplace=True) \n",
    "print(rules_support_family.head(3))\n",
    "\n",
    "# Defining Confidence\n",
    "rules_confidence_family = association_rules(frequent_itemsets_family, metric=\"confidence\", min_threshold=0.50) \n",
    "rules_confidence_family.sort_values(by='confidence', ascending=False, inplace=True) \n",
    "print(rules_confidence_family.head(3))\n",
    " \n",
    "# Defining Lift\n",
    "rules_lift_family = association_rules(frequent_itemsets_family, metric=\"lift\", min_threshold=0)  \n",
    "rules_lift_family.sort_values(by='lift', ascending=False, inplace=True) \n",
    "rules_lift_family.head(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179038ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First draft.\n",
    "# Plot a basic network graph of the top 15 confidence rules\n",
    "\n",
    "# Create a copy of the rules and transform the frozensets to strings\n",
    "rulesToPlot_family = rules_confidence_family.copy(deep=True)\n",
    "rulesToPlot_family['LHS'] = [','.join(list(x)) for x in rulesToPlot_family['antecedents']]\n",
    "rulesToPlot_family['RHS'] = [','.join(list(x)) for x in rulesToPlot_family['consequents']]\n",
    "\n",
    "# Remove duplicate if reversed rules\n",
    "rulesToPlot_family['sortedRow'] = [sorted([a,b]) for a,b in zip(rulesToPlot_family.LHS, rulesToPlot_family.RHS)]\n",
    "rulesToPlot_family['sortedRow'] = rulesToPlot_family['sortedRow'].astype(str)\n",
    "rulesToPlot_family.drop_duplicates(subset=['sortedRow'], inplace=True)\n",
    "\n",
    "# Plot Graphically\n",
    "rulesToPlot_family=rulesToPlot_family[:15]\n",
    "fig = plt.figure(figsize=(17, 17)) \n",
    "G = nx.from_pandas_edgelist(rulesToPlot_family, 'LHS', 'RHS')\n",
    "np.random.seed(1000)\n",
    "nx.draw(G, with_labels=True, \n",
    "        node_size=30, \n",
    "        node_color=\"red\", \n",
    "        pos=nx.spring_layout(G))\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b302413a",
   "metadata": {},
   "source": [
    "- **4.6) Modelling: By Family**\n",
    "- 4.6.2) Key Takeaways<br>\n",
    "<br>\n",
    "- **Most frequent consequents:** Rice.\n",
    "- All the dots that are closer to each of these \"centers\", are the most usual antecedents.\n",
    "- The closer is the point, the higher is the confidence level.\n",
    "- The two closest antecedents are: {Holds, Sizzling and Drinks} and {White Wine, Sizzling, Meat and Starters}.\n",
    "- In the end, we decided do not take into account this results in our final recommendations, since we believe that products provide us a much more detailed explanation of costumers behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823a01aa",
   "metadata": {},
   "source": [
    "- **4.7) Menus Suggestions**\n",
    "- 4.7.1) Menus for Dinne-Inns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ce12d1",
   "metadata": {},
   "source": [
    "- **Itemset:** \"list of all the items in the antecedent and the consequent\".\n",
    "- **Support:** \"the fraction of the total number of transactions in which the itemset occurs (%)\". \n",
    "- **Antecedent Support:** represents the frequency of the antecedent itemset in the dataset (%).\n",
    "- **Consequent Support:** represents the frequency of the consequent itemset in the dataset (%).\n",
    "- **Confidence:** \"the conditional probability of occurrence of consequent given the antecedent (%)\".\n",
    "- **Lift:** \"the conditional probability of occurrence of consequent given the antecedent\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aa1d11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Measuring the quantity of products per set.\n",
    "\n",
    "frequent_itemsets_dinne_in_no_water['products_per_set'] = frequent_itemsets_dinne_in_no_water['itemsets'].apply(lambda x: len(x))\n",
    "\n",
    "\n",
    "# Checking the results for a 'products_per_set' = 2 and a Support >= 0.1.\n",
    "\n",
    "# Reminding, a support repreents the percentage of transactions in the entire dataset that contain a particular itemset. \n",
    "# In this case, we aim to filter all the itemsets that represents at least 10% of the total transactions.\n",
    "\n",
    "frequent_itemsets_dinne_in_no_water[(frequent_itemsets_dinne_in_no_water['products_per_set'] == 2) & \n",
    "                                    (frequent_itemsets_dinne_in_no_water['support'] >= 0.17)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9fcc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Itemsets with both high 'Confidence' and high 'Support'.\n",
    "# This indicates that many customers purchase A and B together, \n",
    "# and customers who purchase A are highly likely to also purchase B.\n",
    "# Together, a high support and high confidence for an association rule indicate that \n",
    "# the antecedent and consequent occur together frequently, \n",
    "# and that when the antecedent occurs, the consequent is highly likely to occur as well.\n",
    "\n",
    "rules_support_dinne_in_no_water[(rules_support_dinne_in_no_water['confidence'] >= 0.60) &\n",
    "                                   (rules_support_dinne_in_no_water['support'] >= 0.20)\n",
    "                                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee5c675",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_support_dinne_in[(rules_support_dinne_in['confidence'] >= 0.90) &\n",
    "                                   (rules_support_dinne_in['support'] >= 0.20)\n",
    "                                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d726c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Itemsets with both high 'Confidence' and high 'Lift'.\n",
    "\n",
    "rules_confidence_dinne_in_no_water[(rules_confidence_dinne_in_no_water['lift'] >= 2.5) & \n",
    "                                   (rules_confidence_dinne_in_no_water['confidence'] >= 0.6)\n",
    "                                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19195625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are assessing if there are products that customers tend not to buy together, \n",
    "# once they bought one of them.\n",
    "# As we can see below, there are no substitute products in Dinne - Inn.\n",
    "\n",
    "rules_lift_dinne_in_no_water[rules_lift_dinne_in_no_water.lift<=1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8550851c",
   "metadata": {},
   "source": [
    "**Relationships analysis | Suggestions for Dinne - Inns**\n",
    "<br>\n",
    "-  **Substitute Products:** there are no substitute products (since there is no negative lift), which means that we do not suggest any product deletion from the menu.  \n",
    "-  **Creation of new products:** We suggest the creation of a new product of Noodles without meat. If there is a confidence of 100%, it means that when people order **\"No Meat\"**, they also order **\"Noodles with meat\"**, which makes sense. However, if we see the other way around, we check that 67% of the times, if the costumer order **\"Noodles with meat\"**, also order **\"No meat\"**, which is very relevant. Additionally, a lift of ~5.4, which means that the possibility of order **Noodles with meat** is 5.4 times higher than if they were independent.\n",
    "-  **Creation of new menu 1:** **Spring roll** with **Egg fried rice** (support>0.2 and confidence>0.6) and with **Mineral water 1.5lt** (support>0.33 and confidence>0.94 with each of the previous refered products).\n",
    "-  **Creation of new menu 2:** **Sweet sour chicken** with **Egg fried rice** (support>0.2 and confidence>0.6) and with **Mineral water 1.5lt** (support>0.28 and confidence>0.93 with each of the previous refered products).\n",
    "-  **Increse Prices:** There is lift above 5 and a confidence above 0.6, on the items **Naan - Jira pulao** and **Chick tikka masala - Naan**. **Naan** is a starter, **jira pulao** is a side dish and **Chick tikka masala** a main indian dish. So, here there are two approaches we can follow: promote a discount on the the starter **Naan** and then, slightly increase the main and the side dish. Or, in case this leads to a decrease in the demand for main and side dish, we can also think about a menu 3.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Additional Recommendations**\n",
    "<br>\n",
    "-  **Saturday Lunch Campaign:** We have noticed above that Sunday lunch, represents an outlier both in terms of orders volume but also in proportional to the dinner sales volume. However, we do no verify the same on Saturday. In this sense, we propose a possible campaign: if you order on Saturday Lunch before 4pm, we offer you a discount on next dinner visit.\n",
    "- **Best Holidays Promotion:** The three most profitable holidays are separated by 13 days: 24.12.2018 (Christmas Eve), 31.12.2018 (New Year's Eve) and 06.01.2018 (Epiphany). We suggest the adoption of agressive marketing strategies for those specific days: first, the business hours can be slightly extended since we know that there must be high demand; second, we can create special products (for examples, a discount on the most expensive starters and desserts, since the in a special occasition, the costumers may be willing to spend more); and third, think about some partnership nearby the restaurant: festivals, cinema and massage center, for example.\n",
    "- **Worst Holidays Promotion:** For the worst holidays, we suggest a campaign to attract costumers. For example: if you come on one of this holidays, you receive a 50% discount on the next visit in May or June. The holidays are the following: 1.05.2018 (Labour Day/May Day) and (28.05.2018 (Orthodox Pentecost Monday))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c90b0c1",
   "metadata": {},
   "source": [
    "- **4.7) Menus Suggestions**\n",
    "- 4.7.2) Menus for Deliveries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4954cf",
   "metadata": {},
   "source": [
    "- **Itemset:** \"list of all the items in the antecedent and the consequent\".\n",
    "- **Support:** \"the fraction of the total number of transactions in which the itemset occurs (%)\". \n",
    "- **Antecedent Support:** represents the frequency of the antecedent itemset in the dataset (%).\n",
    "- **Consequent Support:** represents the frequency of the consequent itemset in the dataset (%).\n",
    "- **Confidence:** \"the conditional probability of occurrence of consequent given the antecedent (%)\".\n",
    "- **Lift:** \"the conditional probability of occurrence of consequent given the antecedent\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1376883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measuring the quantity of products per set.\n",
    "\n",
    "frequent_itemsets_delivery['products_per_set'] = frequent_itemsets_delivery['itemsets'].apply(lambda x: len(x))\n",
    "\n",
    "\n",
    "# Checking the results for a 'products_per_set' = 2 and a Support >= 0.1.\n",
    "# Reminding, a support repreents the percentage of transactions in the entire dataset that contain a particular itemset. \n",
    "# In this case, we aim to filter all the itemsets that represents at least 10% of the total transactions.\n",
    "\n",
    "frequent_itemsets_delivery[(frequent_itemsets_delivery['products_per_set'] == 2) & \n",
    "                                    (frequent_itemsets_delivery['support'] >= 0.1)\n",
    "                                   ]\n",
    "\n",
    "rules_support_delivery[(rules_support_delivery['support'] >= 0.2) & \n",
    "                                   (rules_support_delivery['confidence'] >= 0.50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7baf366",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Itemsets with both high 'Confidence' and high 'Lift'.\n",
    "rules_confidence_delivery[(rules_confidence_delivery['lift'] >= 1.5) & \n",
    "                                   (rules_confidence_delivery['confidence'] >= 0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a24120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are assessing if there are products that customers tend not to buy together, \n",
    "# once they bought one of them.\n",
    "# As we can see below, there are no substitute products in Deliveries\n",
    "\n",
    "rules_lift_delivery[rules_lift_delivery.lift<=1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a01a2ea",
   "metadata": {},
   "source": [
    "**Relationships analysis | Suggestions for Deliveries**\n",
    "<br>\n",
    "-  **Substitute Products:** there are no substitute products (since there is no negative lift), which means that we do not suggest any product deletion from the menu.  \n",
    "-  **Creation of new products:** We suggest the creation of a new product of Noodles without meat. If there is a confidence of 100%, it means that when people order **\"No Meat\"**, they also order **\"Noodles with meat\"**, which makes sense. Additionally, a lift of ~8, which means that the possibility of order **Noodles with meat** is ~8 times higher than if they were independent.\n",
    "-  **Creation of new menu 1:** **Sweet sour chicken** with **Egg fried rice** (support>0.2 and confidence>0.6).\n",
    "-  **Increse Prices 1:** There is lift above 4.5 and a confidence above 0.54 on the items **Jira pulao - Naan**. **Naan** is an indian starter, **jira pulao** is an indian side dish. So, here, in opposition to what we suggested in dinne-in, there is no main dish so that we could link the products. Anyway, promote a discount on the the starter **Naan** and increase the price of the side dish could be a possibility. \n",
    "-  **Increse Prices 2:** There is lift above 8.2 and a confidence above 0.70 on the items **extra pancakes - extra sauce**, naturally. Here, we could explore a promotion on the price of additional **pancakes**, along with an increase in the **extra source** price, for example. \n",
    "\n",
    "\n",
    "**Additional Recommendations**\n",
    "<br>\n",
    "-  **Saturday Lunch Campaign:** We have noticed above that Sunday lunch, represents an outlier both in terms of orders volume but also in proportional to the dinner sales volume. However, we do no verify the same on Saturday. In this sense, we propose a possible campaign: if you order on Saturday Lunch before 4pm, we offer you a discount on next dinner visit.\n",
    "- **Best Holidays Promotion:** The three most profitable holidays are separated by 13 days: 24.12 (Christmas Eve), 31.12 (New Year's Eve) and 06.01 (Epiphany). We suggest the adoption of agressive marketing strategies for those specific days: first, the business hours can be slightly extended since we know that there must be high demand; second, we can create special products (for examples, a discount on the most expensive starters and desserts, since the in a special occasition, the costumers may be willing to spend more); and third, think about some partnership nearby the restaurant: festivals, cinema and massage center, for example.\n",
    "- **Worst Holidays Promotion:** For the worst holidays, we suggest a campaign to attract costumers. For example: if you come on one of this holidays, you receive a 50% discount on the next visit in May or June. The holidays are the following: 1 of May (Labour Day/May Day) and 28 of May (Orthodox Pentecost Monday)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29edd5fa",
   "metadata": {},
   "source": [
    "# **5. Deployment**\n",
    "\n",
    "\n",
    "- **5.1) Final Recommendation System for Dinne-Inns**\n",
    "- **5.2) Final Recommendation System for Deliveries**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9b6cf5",
   "metadata": {},
   "source": [
    "- **5.1) Final Recommendation System for Dinne-Inns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a22fdaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the order as a dinner in case the invoice was issued after 6pm.\n",
    "df[\"Dinner_meal\"] = [0 if x.hour < 18 else 1 for x in df.InvoiceDateHour]\n",
    "\n",
    "# Creating the bins for the number of people per order.\n",
    "df[\"People_agg\"] = [\"1\" if i == 1 else \n",
    "                    \"2\" if i == 2 else \n",
    "                    \"3\" if i == 3 else \n",
    "                    \"4-5\" if i <= 5 else \n",
    "                    \"5-10\" if i <= 10 else \n",
    "                    \"10+\" for i in df.Pax ]\n",
    "\n",
    "# Check sizes of customer segments\n",
    "df_dinne_inns = df[df.IsDelivery == 0]\n",
    "ct = pd.crosstab(index=[df_dinne_inns.People_agg],\n",
    "                 columns=[df_dinne_inns.Dinner_meal, df_dinne_inns.Holiday])\n",
    "\n",
    "# Sort by People_agg\n",
    "ct_sorted = ct.sort_index(level=0)\n",
    "ct_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbde9dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T13:52:43.395803Z",
     "start_time": "2020-03-20T13:52:43.323900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Auxiliar functions:\n",
    "    # 1. df_people\n",
    "    # 2. df_products_alphabetically\n",
    "    # 3. df_rules\n",
    "\n",
    "    \n",
    "# 1.This function returns the transactions dataset filtered by the following variables: \n",
    "    # 'Deliveries', \n",
    "    # 'People_agg'\n",
    "    # 'Dinner_meal'.\n",
    "    # 'Holidays'\n",
    "def df_people(People_agg):\n",
    "    \n",
    "    # 1.1 Obtain the bins for people category.\n",
    "    if People_agg <= 1: \n",
    "        People = \"1\"\n",
    "    elif People_agg <=2:\n",
    "        People = \"2\"\n",
    "    elif People_agg <= 5:\n",
    "        People = \"3-5\"\n",
    "    elif People_agg <= 10:\n",
    "        People = \"5-10\"\n",
    "    else:\n",
    "        People = \"10+\"\n",
    "        \n",
    "    # 1.2 Get current time and check if dinner.\n",
    "    dinner = 1                      # By default, we are assuming that the meal is a dinner, since the majority of the meals are dinner.\n",
    "    if dt.datetime.now().hour < 18: # By previously looking at the plots, we have defined this as our threshold.\n",
    "        dinner = 0 \n",
    "        \n",
    "    # 1.3 Get current date and check if holiday\n",
    "    holiday_day = 0\n",
    "    if len(df[(df.Holiday == 1) & (df.month == dt.datetime.now().month) & (df.day == dt.datetime.now().day)]) != 0:\n",
    "        holiday_day = 1\n",
    "    \n",
    "    # 1.4 Making our final filter, storing in a new variable, called df_finne_inns. \n",
    "    df_dinne_inns = df[(df.IsDelivery == 0) & \n",
    "                       (df.Dinner_meal == dinner) & \n",
    "                       (df.Holiday == holiday_day) &\n",
    "                       (df.People_agg == People)].copy(deep=True)\n",
    "    return df_dinne_inns\n",
    "\n",
    "\n",
    "# 2. This function returns a list of alphabetically sorted strings with all possible itemset combinations\n",
    "def df_productset_alphabetically(products):\n",
    "    \n",
    "     # Get all possible combinations as a list of tuples.\n",
    "    product_sets = []\n",
    "    for i in range(1,len(products) + 1):\n",
    "        for i in itertools.combinations(products, i):\n",
    "            product_sets.append(i)\n",
    "\n",
    "    # Transform tuples into list of strings, sort the list alphabetically and create string out of sorted list .\n",
    "    productslists = [list(x) for x in product_sets]\n",
    "    for i in productslists:\n",
    "        i.sort()\n",
    "    productsstrings = [\" \".join(x) for x in productslists]\n",
    "    return productsstrings\n",
    "\n",
    "\n",
    "# 3. This functions returns a dataframe with rules, based on the df returned by filter_df(pax)\n",
    "def df_rules(People_agg, support_min, confidence_min):\n",
    "    \n",
    "    # Create frequent itemset of filtered df\n",
    "    df_dinne_inn = pd.pivot_table(df_people(People_agg)[[\"DocNumber\",\"ProductDesignation\"]], \n",
    "                            index='DocNumber', \n",
    "                            columns='ProductDesignation', \n",
    "                            aggfunc=lambda x: 1 if len(x)>0 else 0).fillna(0) \n",
    "    frequent_itemsets_df_dinne_inn = apriori(df_dinne_inn, min_support=support_min, use_colnames=True) \n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=confidence_min)\n",
    "    \n",
    "    # Remove duplicate if reversed rules\n",
    "    rules['LHS'] = [','.join(list(x)) for x in rules['antecedents']]\n",
    "    rules['RHS'] = [','.join(list(x)) for x in rules['consequents']]\n",
    "    rules['sortedRow'] = [sorted([a,b]) for a,b in zip(rules.LHS, rules.RHS)]\n",
    "    rules['sortedRow'] = rules['sortedRow'].astype(str)\n",
    "    rules.drop_duplicates(subset=['sortedRow'], inplace=True)\n",
    "\n",
    "    # Transform antecedents and consequents into list of strings, sort the list alphabetically and create string out of sorted list \n",
    "    rules[\"antecedents\"] = rules[\"antecedents\"].apply(lambda x: list(x))\n",
    "    rules[\"consequents\"] = rules[\"consequents\"].apply(lambda x: list(x))\n",
    "\n",
    "    for i in rules.index.values:\n",
    "        rules.loc[i,\"antecedents\"].sort()\n",
    "        rules.loc[i,\"consequents\"].sort()\n",
    "\n",
    "    rules[\"antecedents\"] = rules[\"antecedents\"].apply(lambda x: \" \".join(x))\n",
    "    rules[\"consequents\"] = rules[\"consequents\"].apply(lambda x: \" \".join(x))\n",
    "\n",
    "    # Remove space in the beginning of antecedents and consequents\n",
    "    rules[\"antecedents\"] = rules[\"antecedents\"].apply(lambda x: x[1:] if x[0] == \" \" else x)\n",
    "    rules[\"consequents\"] = rules[\"consequents\"].apply(lambda x: x[1:] if x[0] == \" \" else x)\n",
    "    \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96915c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T13:52:43.419772Z",
     "start_time": "2020-03-20T13:52:43.399799Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get recommended products \n",
    "# Based on number of persons and ordered items, return maximum 7 itemsets with the highest lift\n",
    "\n",
    "def final_recommendation_dinne_inn(People_agg, products, support_min=0.05, confidence_min=0.15, lift_min=1.1):\n",
    "    productsstrings = df_productset_alphabetically(products)\n",
    "    rules = df_rules(People_agg, support_min, confidence_min)\n",
    "    \n",
    "    # Filter possible recommendations\n",
    "    suggestions = rules[rules.antecedents.isin(productsstrings)]\n",
    "    \n",
    "    # Sort by lift and select relevant information of top ten\n",
    "    best_seven = suggestions.sort_values(\"lift\", ascending=False)[:7][[\"antecedents\", \"consequents\", \n",
    "                                                                   \"support\", \"confidence\", \"lift\"]]\n",
    "    \n",
    "    # Only consider rules with certain lift \n",
    "    best_seven = best_seven[best_seven.lift >= lift_min]\n",
    "    \n",
    "    return best_seven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b779778",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T13:52:53.647129Z",
     "start_time": "2020-03-20T13:52:43.423766Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example: \n",
    "    # Two costumers that orders \"Mineral water 1.5lt\": \n",
    "        # \"What are the main suggestions under our model?\"\n",
    "\n",
    "final_recommendation_dinne_inn(2, [\"Mineral water 1.5lt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc98713f",
   "metadata": {},
   "source": [
    "- **5.2) Final Recommendation System for Deliveries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18deedb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the order as a dinner in case the invoice was issued after 6pm.\n",
    "df[\"Dinner_meal\"] = [0 if x.hour < 18 else 1 for x in df.InvoiceDateHour]\n",
    "\n",
    "# Check sizes of customer segments\n",
    "df_deliveries = df[df.IsDelivery == 1]\n",
    "pd.crosstab(df_deliveries.CustomerCity,[df_deliveries.Dinner_meal, df_deliveries.Holiday])\n",
    "# Sort by People_agg\n",
    "ct_sorted_deliveries = ct.sort_index(level=0)\n",
    "ct_sorted_deliveries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338424f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T13:52:43.395803Z",
     "start_time": "2020-03-20T13:52:43.323900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Auxiliar functions:\n",
    "    # 1. df_city_delivery\n",
    "    # 2. df_products_alphabetically_delivery\n",
    "    # 3. df_rules_delivery\n",
    "\n",
    "# 1.This function returns the transactions dataseet filtered by:\n",
    "    # Only Delivery customers\n",
    "    # Corresponding city\n",
    "    # Whether is Holiday or not\n",
    "    # Whether is Dinner or not\n",
    "def df_city_delivery(city):\n",
    "    # Get current time and check if dinner\n",
    "    dinner_meal = 1\n",
    "    if dt.datetime.now().hour < 18:\n",
    "        dinner_meal = 0\n",
    "        \n",
    "    # Get current date and check if holiday\n",
    "    holiday_day = 0\n",
    "    if len(df[(df.Holiday == 1) & (df.month == dt.datetime.now().month) & (df.day == dt.datetime.now().day)]) != 0:\n",
    "        holiday_day = 1\n",
    "    \n",
    "    # Filter dataset \n",
    "    df_deliveries_1 = df[(df.IsDelivery == 1) & \n",
    "                         (df.Dinner_meal == dinner_meal) & \n",
    "                         (df.Holiday == holiday_day) & \n",
    "                         (df.CustomerCity == city)].copy(deep=True)\n",
    "    if len(df_deliveries_1.index.values) > 75:\n",
    "        return df_deliveries_1\n",
    "    else: \n",
    "        return df[(df.IsDelivery == 1) & \n",
    "                  (df.Dinner_meal == dinner_meal) & \n",
    "                  (df.Holiday == holiday_day)].copy(deep=True)\n",
    "\n",
    "# 2. This function returns a list of alphabetically sorted strings with all possible itemset combinations\n",
    "def df_products_alphabetically_delivery(products):\n",
    "    \n",
    "     # Get all possible combinations as a list of tuples\n",
    "    productsets = []\n",
    "    for i in range(1,len(products) + 1):\n",
    "        for i in itertools.combinations(products, i):\n",
    "            productsets.append(i)\n",
    "\n",
    "    # Transform tuples into list of strings, sort the list alphabetically and create string out of sorted list \n",
    "    productslists = [list(x) for x in productsets]\n",
    "    for i in productslists:\n",
    "        i.sort()\n",
    "    productsstrings = [\" \".join(x) for x in productslists]\n",
    "    return productsstrings\n",
    "\n",
    "# 3. This functions returns a dataframe with rules, based on the df returned by filter_df(pax)\n",
    "def df_rules_delivery(city, support_min, confidence_min):\n",
    "    \n",
    "    # Create frequent itemset of filtered df\n",
    "    df_deliveries = pd.pivot_table(df_city_delivery(city)[[\"DocNumber\",\"ProductDesignation\"]], \n",
    "                            index='DocNumber', \n",
    "                            columns='ProductDesignation', \n",
    "                            aggfunc=lambda x: 1 if len(x)>0 else 0).fillna(0) \n",
    "    frequent_itemsets = apriori(df_deliveries, min_support=support_min, use_colnames=True) \n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=confidence_min)\n",
    "    \n",
    "    # Remove duplicate if reversed rules\n",
    "    rules['LHS'] = [','.join(list(x)) for x in rules['antecedents']]\n",
    "    rules['RHS'] = [','.join(list(x)) for x in rules['consequents']]\n",
    "    rules['sortedRow'] = [sorted([a,b]) for a,b in zip(rules.LHS, rules.RHS)]\n",
    "    rules['sortedRow'] = rules['sortedRow'].astype(str)\n",
    "    rules.drop_duplicates(subset=['sortedRow'], inplace=True)\n",
    "\n",
    "    # Transform antecedents and consequents into list of strings, sort the list alphabetically and create string out of sorted list \n",
    "    rules[\"antecedents\"] = rules[\"antecedents\"].apply(lambda x: list(x))\n",
    "    rules[\"consequents\"] = rules[\"consequents\"].apply(lambda x: list(x))\n",
    "\n",
    "    for i in rules.index.values:\n",
    "        rules.loc[i,\"antecedents\"].sort()\n",
    "        rules.loc[i,\"consequents\"].sort()\n",
    "\n",
    "    rules[\"antecedents\"] = rules[\"antecedents\"].apply(lambda x: \" \".join(x))\n",
    "    rules[\"consequents\"] = rules[\"consequents\"].apply(lambda x: \" \".join(x))\n",
    "\n",
    "    # Remove space in the beginning of antecedents and consequents\n",
    "    rules[\"antecedents\"] = rules[\"antecedents\"].apply(lambda x: x[1:] if x[0] == \" \" else x)\n",
    "    rules[\"consequents\"] = rules[\"consequents\"].apply(lambda x: x[1:] if x[0] == \" \" else x)\n",
    "    \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2394719c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T13:52:43.419772Z",
     "start_time": "2020-03-20T13:52:43.399799Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get recommended products \n",
    "# Based on number of city and ordered products, return maximum 7 itemsets with the highest lift.\n",
    "def final_recommendation_delivery(city, products, support_min=0.03, confidence_min=0.1, lift_min=1.1):\n",
    "    productsstrings_delivery = df_products_alphabetically_delivery(products)\n",
    "    rules_delivery = df_rules_delivery(city, support_min, confidence_min)\n",
    "    \n",
    "    # Filter possible recommendations\n",
    "    suggestions = rules_delivery[rules_delivery.antecedents.isin(productsstrings_delivery)]\n",
    "    \n",
    "    # Sort by lift and select relevant information of top ten\n",
    "    best_seven = suggestions.sort_values(\"lift\", ascending=False)[:7][[\"antecedents\", \"consequents\", \n",
    "                                                                        \"support\", \"confidence\", \"lift\"]]\n",
    "    \n",
    "    # Only consider rules with certain lift \n",
    "    best_seven = best_seven[best_seven.lift >= lift_min]\n",
    "    \n",
    "    return best_seven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d0d3e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T13:52:53.647129Z",
     "start_time": "2020-03-20T13:52:43.423766Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Example: \n",
    "    # A costumer that orders \"Spring roll\" in \"Egkomi\":\n",
    "        # \"What are the main suggestions under our model?\"\n",
    "\n",
    "final_recommendation_delivery(\"Egkomi\", [\"Spring roll\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
